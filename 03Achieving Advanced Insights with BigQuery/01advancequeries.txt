# Advanced functions (Statistical, analytic, user-defined)
Welcome back. This is the third course in the data analysts specialization. Here we're going to achieve advanced insights in BigQuery. For the first module, when we continue our journey in SQL by delving into a few of the more advanced concepts like statistical approximation functions and user-defined functions. Then we will explore how to break apart really complex data questions into step-by-step modular pieces in SQL by using common table expressions and sub-queries. I'll start by revisiting the SQL functions that we've covered so far. It's time to revisit some of those functions that we skipped over in previous courses as we move into more and more advanced SQL concepts. The three ones that are on the agenda are the statistical functions, which includes some pretty fun approximations. Analytical functions, if you've heard of things like window functions or partitioning in the query. Last but not least, UDFs or user-defined functions. They can be written in SQL or JavaScripts. Let's tackle them one at a time. First up, some pretty generic statistical functions that you can do on your data. If you imagine a standard deviation or a correlation analysis. This is a great example just to apply some basic statistics. Here we're looking at the IRS dataset and we're getting the standard deviation of the employee counts. Also for the second correlation, we're passing in. How correlated is the revenue that one of these charities brings in, two, the amount of functional expenses that it's actually spending on their charitable programs and offerings. That's the ultimate question is how correlated do you think those two are? The closer it is to one, naturally, that is going to be a very tight one-to-one correlation. Just running that SQL there. You'll see the value here, standard deviation for about 1,500 employees. Then you can see that revenue and expenses for these non-profit charities are very tightly correlated. Again, this is for all of those records in their 2015 file and dataset, with that 0.97, extremely highly correlated. That's enough for the basic statistical functions. Let's move into some more of the other worldly ones. Here's some approximate aggregation functions. If you wanted to approximate the amount of data that's in your dataset and you say you have an absurd amount of rows. What you can use is approximate count distinct. Much like you've seen before, with the count distinct there in that second line, you can get that exact count. But you can also get the approximation which can be good enough to get the job done. Now you thinking in the back of your head like, well, the count distinct ran in under five seconds for my dataset that we were doing in our labs. Why on earth would I ever want to use the approximate count distinct and get the exact count. Me coming from the finance background if we wanted everything down to the penny. But imagine you're in this scenario where you work at Google and you have to count all of the logs or all the user logins over a multi-year period of time. You're talking about petabytes of data and you don't have all the time in the world. You want to actually sacrifice a little bit of accuracy for speed in which you get your query back. Especially when you're talking about counting all the logs that are processed or counting all the ads that are served. Very popular to do that approximate count distinct. Let's look at an example here. This is an example from one of our BigQuery Developer Advocates. Steven, he's an amazing SQL writer. What we're going to invoke here is that approximate count distinct. The task that we have at hand is the Github dataset is public on BigQuery, and that includes all of the user logins and all the public code on Github. That's a very fun public dataset to play around with. Here what we're doing is we're counting the approximate number of logins as an approximate value. What we're looking at is to see it by year. Here you'll see those concepts that we covered in previous courses where you have the union table wildcard there, that's that asterisk from Steven and then the concatenation in that table suffix again, is one of those concepts that we covered in the merging datasets topic. We now have an approximate count of users. But what you might be asking in the back of your minds, is how accurate is it is that to like the actual count and say, we wanted to get a more realistic count of all of the users since the period of time. Because this data right here could include the same user that is logged in over multiple years. How do we parse that out? To do that, it's a little bit more of a complex query, probably the most complex one that you've seen to date as part of this specialization. Let's take a deep breath and just walk through it step by step, line by line. The first thing that you're going to notice is we're invoking what's called a common table expression all the way at the top. We're going to cover that width clause shortly. But in essence, you can think of that as a temporary table. It's basically saying anything in those parentheses for that width, Github use sketches, anything within there, that select statement. We're going to be storing that like a temporary table that we can use in a subsequent query. The reason why you do that again is to break apart a very complex question into multiple steps. You can do a processing layer on the data and then query those results within that same query. Those width statements or common table expressions as they're called are extremely useful. Let's get into the guts of the query. That first query, what we want do is we want to get the approximate number of logins. What we're going to do is invoke a function called the HyperLogLog for another approximation. What that's going to do is it's going to also estimate the amount of logins just using a different function that's available. There's a long white paper written on HyperLogLog. It's an externally available algorithm and it's used all the time in things like statistical estimation as well. Once we've done that, the great thing about this is we can actually estimate year over year over year over year. We're doing this for however many years of Github data that we have on the previous slide, it was like 2,000 to current, and we're getting an estimate year over year over year over year. In the HyperLogLog namespace, those are actually called sketches. We're getting a collection of sketches which in a sense are estimates of counts. In that second block of code at the bottom, we can actually add those sketches together, which is summing those up. You can actually do that through the HyperLogLog count.merge. That merges all those estimations together as an approximate number of those unique users. As you see in the last line there, that's from that countable expression, that Github, your sketches that we created earlier. There's a lot to cover, but at the end of the day it ended up with 11 million approximately unique Github users. Then a different query. We're not showing it here where we actually ran to get the exact count. We found that this approximation was 99.7 percent accurate. Again, here's the trade-off between whether or not you want to get a 100 percent accuracy and have your queries run a little bit longer. Or if you're fine with just executing the approximation functions. If you see these in encode along the way or you start writing these approximations and estimations on very large datasets, just know that those are out there and they're available for you to use. Feel free to read a blog article for a little bit more detail on that. Approximation functions are another tool in your toolkit for very, very large datasets. Feel free to get familiar with them and we'll come back to those width clause is a little bit more.

# Analytic Functions, WITH clause, and RANK()
OK, one of my favorite topics. So this is where we break away from what I call intermediate SQL, which is like joins and unions and getting super familiar with working with nulls and a lot of the topics that we covered as part of that Creating Datasets and Visualizing Insights course. And we really move into some of what I would call the advanced SQL or the beginnings of the advanced SQL concepts, which is using things like ranking and numbering functions. So if you've ever heard of the rank function or people using row number, or dense rank or leading and lagging to actually iterate between rows in the output, this is where we can start to get really dangerous with SQL. So let's take us through a few examples. OK, so this is where we mentioned the concept of a window function. So what ultimately what we want to do here is we start with, all the way in the left, a table of employees at an imaginary company. And they're part of two different departments. Let's say department one is sales. And department two is analytics. And each of the different employees have a tenure at the organization. So this is when they first started. And the question that we want to answer at the end of the day is rank everybody by each department in how long that they've actually been there. Now think about that. We actually have to break apart our original data set, kind of like a group by, right, into two different departments. But then we also have to rank those records. So yeah, what you could do technically is you could do a select from the employee list there and then where department equals one and then do your ranking there via an order by on the start date and then mash that together with a union again for that other department for the department number two there. But that's not as scalable. Say instead of two departments, there were 10 departments. You don't want to keep breaking things apart manually with where clauses and then unioning them back together. And that's why we're going to introduce the analytical window function. And one of them in particular is called RANK. So the SQL we're going to review on the next slide, but let's review conceptually what we're doing. OK, so on the left, we have the list of employees and their tenure. The first thing we have to do is we have to split it apart by department. So that's where we're invoking this thing called partitions. We're partitioning by department. And this is taking one data table. And now we're breaking it apart so we can operate on separate chunks of that. So that's that second column there. And then next, we need to give it a logical ordering. So what's the whole point of actually us breaking this apart? Well, we ultimately have to answer the question of who's been there the longest. In order to do that, we need to get our records in a sensible order before we apply a rank of the rows, so 1, 2, 3, 4, 5, that sort of thing. So that third column of information there has the sorted results there. So you can see Jacob there for department number one has been there the longest since 1990. And Isabella in department two has been in that department since 1997. And last but not least is just actually applying a rank function, which is going to be a new column all the way at the end. And you can see that we're just simply applying the ranks of the rows 1, 2, 3, 4, 5. And there is-- in the case of ties, you could actually use-- for rank, it's going to be 1-1. And then you could say 2, 3, 4. Or if you just wanted the dense rank, one of the functions, I believe it's dense rank, will actually-- ties are still counted as 1-1. But it'll actually skip over and say 1-1 and then 3. So I'm pretty sure that's the rank whereas the dense rank will actually go 1-1 and then 2. Anyways, so recap of what we actually just did starting from left to right. We have an employees table. We're splitting it apart into different subgroups or windows. And then we're ordering on it. And then last but not least, we're applying that ranking function, just one of the available analytical functions. These are called analytical functions that we're using on this data. So what is the actual SQL query look like? Here we go. So we've got the first name of the employee, the department. And again, this is actually how we're going to break apart that singular table into those different chunks, right? Those different chunks we're going to call partitions. And then we've got the start date, which is how we're actually going to get value out of why we're breaking them apart in the first place because we have to answer the question of who has the most tenure at the firm. And that last field is the calculated field there where it's the analytical function that we're invoking. So it's saying perform a rank operation over in parentheses. Here's how we're going to set up those windows of data. Here's how we're going to partition it. So we're going to say partition by the department, meaning break apart that singular table into those two different departments or however many different departments. And while you're at it, store each of those by a sorted order, where it's start date. And again, order by defaults to ascending in this case, and for dates, that means oldest to newest.

# BigQuery User-Defined Functions (UDFs)
One of the common topics that you heard me already warned you about when it comes to pricing and other limitations, but it's potentially powerful. With great power comes great responsibility is the concept of a user-defined function. If the function doesn't exist already inside of BigQuery, cool news is you can create it, or even cooler news is somebody else has already created it in JavaScript and you can just invoke that. Some fun examples that I've seen of really cool user-defined functions that you can just check out on the web, natural language processing, and BigQuery. If you're looking at Tweets or Reddit post comments, and you want to do some sentiment analysis, or seeing the closeness between words, you'd actually do that with some pretty cool JavaScript libraries. Inside of BigQuery, you can invoke those or call those JavaScript libraries if they're stored in a Google Cloud storage bucket. What you can do then after that is then in your actual query, you can invoke those functions. For example, here you see CREATE FUNCTION and the input is a string value, and it also is going to return a string. Pretty simple again. This is something that I would highly recommend doing through SQL concatenation instead of invoking some JavaScript to do it just for performance reasons. But I've seen some pretty complex things that you can do with JavaScript and the two flavors, or if two languages that you can write these user-defined functions in are currently SQL. You can actually do some SQL functions that don't already exist. You can create them in JavaScript as well. If it's not clear yet, there's a drastic potential performance. If you want to see that in an example here, the concurrent rate limit for queries for that don't contain UDFs. You can run 50 queries at the same time, if you don't have UDFs in them, 50 queries at the same time as a ton of queries. But for UDF queries, you're limited to six. You can see there's a drastic quota restriction. For good reason again, you're operating JavaScript, potentially long JavaScript functions that are being called over each of your rows that you're processing those on as well. But it does exist, just wield that power responsibly.

# Sub-query and CTE design
So this is going to be a little bit of a review since we covered the common table expressions which is that with claws. But I'm just going to show you just another example of good code hygiene. So here's the example and we're going to talk through this. So you saw us break apart a Complex query into multiple pieces. So on the left there we're just going to walk through those temporary tables that we're creating by invoking that with claws and try to organize it as neatly as I can starting from the top, right? So use line three is with and they were saying the I R S 9 90 2015 E I N includes all look up details of that table. Because we're going to join on line 11, there is the organizational details. So it's saying give me all records from both the 2015 table and the organizational look up table, right? And we don't want to do that again later on in the code. So after that we have that table, we've identified duplicates in that second temporary table. Line 15 as things that have more than one count or instance of their E I N occurring in the 2015 filing table. Again, keeping in mind here, if your charity filing for the 2015 calendar year, normally you'll have just one tax exemption form the 9.90. So you want to fill throughout the rest, and then the last lines line 27 through 40 is where we pick and choose what fields we want from the 2015 table that we've joined previously. And then we join against the duplicates to basically say, hey, if you're in this duplicate spot, we pretty much want to just filter you out. So it'll be flagged as a duplicate and then line 40 wherever the duplicate is not flagged, the duplicates.E I N we're actually going to say include all those non duplicated records. Okay, but the key takeaway here is you can chain multiple temporary tables together. And what I'll do constantly is I'll look at something like this and say, all right, the query works and it's well written. But then I'll take something like line 63 12 and I'll store that as a permanent table after I've written this massive query and gotten the results that I want. So then you don't have to continuously invoke these temporary tables, right? This is mainly so you can just give your brain some breathing room and breaking apart of very complex query into its sub component parts.
Play video starting at :2:30 and follow transcript2:30
I highly encourage you to to experiment and play with clause and you'll get a chance to win your next lab.
Play video starting at :2:37 and follow transcript2:37
>> All right, let's recap. So you finished covering sequel functions which included some pretty neat ones that they allow you to statistically estimate with great accuracy across huge data sets. Again, it's your option here whether or not you want to trade processing time for 100% accuracy. Next we cover an example, where we wanted to break apart a single table into subgroups of rows that we wanted to perform a ranking calculation on each of those subgroups. We did this using an analytical window function.
Play video starting at :3:3 and follow transcript3:03
After that, we introduced UDFs or user defined functions, which can be written in either a sequel or JavaScript. Keep in mind the caveat here, that query performance is impacted. Lastly and probably what I use the most is make liberal use of that with clause, which allows you to break apart a very complex sequel question into multiple steps.