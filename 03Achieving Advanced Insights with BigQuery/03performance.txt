# Avoid BigQuery Performance Pitfalls

Now here's one of the modules that everybody loves instead of make your queries run faster and save on data processing costs. In this performance optimization module, we'll cover the key types of work that BigQuery does in the back-end and how you can avoid falling into performance traps. Then we'll cover one of the most common data traps, which is having too much data skewed to too few values and how filtering can be your best friend here. Last up is your best tool for analyzing and debugging performance issues, the Query Explanation map. Now there are four key areas that govern a lot of the characteristics of BigQuery performance. Let's take a look at each of them in turn and see some examples. First and foremost is how much data that you're reading and writing out of BigQuery, you have your input and output. Second is that communication between your workers or slots. It's done via the shuffle mechanism that we talked a little bit about before and the architecture lecture. Third is highly computational work like LARGE functions, UDFs mathematical operations, that's how much of the CPU you're using. The fourth, which is my favorite is the SQL syntax. Is there more efficient ways to write your queries? Or what are some of those pitfalls that we can avoid when we're writing that SQL. Let's look at each of those. First up is input and output. How much data we're reading and then how much data we're writing. As you might have expected, select star makes the list here. We only want to use the columns that we're actually going to use. If you want a quick preview of that dataset, again, use the preview mechanic that's built into the metadata and the BigQuery UI. Second, BigQuery performs the best when your data is denormalized. That means instead of having a lot of smaller tables and doing join to bring the data back together, have all those DataTables be combined into one massive table or a denormalized table, as we saw in the architecture lecture, and use things like nested and repeated fields instead of having orders in one table and customers in the other, consider actually bringing those together in a parent-child relationship through that nested and repeated structure that we looked at it a little bit earlier. Lastly, as you saw on the merging and union of datasets, try to use those really granular suffixes in your table wildcards to get the most specificity out of your data. As an example here, if you remember that GSOD or JSON tables for weather. Having something like GSOD star is little bit less specific than saying GSOD20 star to get only the years from the year 2000 on forward. Next, let's talk a little bit about BigQuery native storage versus the external data connections that we discussed a little bit more ingesting those datasets. Let's cover some of the downfalls of external data connections. First, they'll never be able to be cached because caching as a function of BigQuery managed storage itself. Second, live edits to underlying data sources. If there's a spreadsheet that you're relying on data from just to quickly pull that into BigQuery. If you have somebody that's editing that spreadsheet live at the same time, you could create potential race conditions and BigQuery is not going to know which dataset is the latest to bring in from that spreadsheet versions. Lastly, BigQuery has a lot of performance tweaks under the hood that you might not even realize. When you're writing things like a long query, if you filter your dataset all the way at the end, what BigQuery might actually do is it recognizes that filter that can actually be performed earlier, and it'll do something that's called predicate push-down. Without even you knowing it, it'll actually bump up your WORK clause much earlier in the query and then filter that data before it gets processed a little bit. Now keep in mind there are particular use cases where you want to use an external data connection, like grabbing in ad hoc data and then performing a onetime analysis on a much like in your extract, transform, and load step, and then ultimately storing it. You don't have to continuously do that. But keep in mind the performance hits and the trade-offs that you're making there. Next up is this shuffle mechanic. One of the most interesting things behind the hood is how those workers communicate to each other to break apart your query and assign that work to a lot of different workers or slots behind the scenes. Now we can do to make their lives easier as pre-fill to your data before doing massive joins, see if you have 80 percent of your records are null, go ahead and lab off those rows even before passing that work to the workers. That saves a lot of reads and it saves a lot of communication time passing those data between each of the different slots. Now one of the things I'll highlight here is that you can identify potential data skew if you have one of your workers that it's getting particularly bottlenecked. We'll take a look at how you can identify that and recognize that a little bit down the road. Here's a few fun querying examples. When we talked about the WITH clause, we said use it liberally and help break apart those complex queries. But one of the caveats or drawbacks to that is you don't want to use a bunch of WITH clause in place of materializing the results of those queries into permanent storage. Here you see in the query on the left, we're using a WITH clause to define a join against the IRS 2015 filings data against the organizational details table. If you ran this query more than once, you'll continuously doing that joint over and over and over again. It's much more performance just to store lines 6-13 as a permanent table and then refer to that later. Now the main takeaway with WITH clauses is that the results of those queries, those subqueries that you've named in that WITH clause like line 6 and line 15 here, those results are not materialized and they're actually requerried every single time if you're referencing those tables more than once of one of your long queries. It's a trade-off between readability for your code and also performance or speed when you're storing those tables as permanent. Here's another example that demonstrates that same concept. For example, if you're only interested in looking at the charities that are schools in the IRS 2015 dataset, and you're commonly passing this filter over and over and over again, it's better to actually store those results as a permanent table and then query those directly. Now let's revisit our friend, the GROUP BY clause. If you remember, the GROUP BY actually aggregates those values over this subset of data that you have. In this particular case, we're looking at the number of Wikipedia contributors and then grouping the amount of edits that they made on the amount of IDs or editors that there are for that Wikipedia dataset. If you can imagine there's a ton of different contributors and doing such a large group by potentially creates many what we call forced shuffles or forced communication steps between your underlying slots or workers. One of the ways you could get around this is potentially performing another level or high level aggregations on your dimension, the contributor here to bucket them. You have fewer buckets that you're grouping by. We'll look at this example in greater depth in just a few slides. Moving on to the computational aspect of BigQuery, if you wanted to optimize the CPU usage and you're getting resources exceeded errors, take a look at any JavaScript user-defined functions you might have and see if there's any way to revisit those concepts within native SQL functions or potentially SQL digital file functions. The reason being here is that BigQuery actually has to fire up a Java sub-process to run a lot of your JavaScript code. That doesn't mean you shouldn't use UDFs. The UDFs have their place when there is functions that are just outside of the normal scope of SQL capabilities. But just keep in mind that performance trade-off there. Now as we discussed a lot in the joins and unions lecture, keep in mind that you want to fundamentally master your data model on how your data tables are related to each other. The relationship between customers and orders, one customer can have many orders, or one order can only belong to one customer. Going through that mental exercise like what we did with the weather stations and the temperature data, will help you determine whether or not the results sets that you get makes sense. Now one of the specific examples that we looked at earlier was uncovering the insight that one particular charity filed more than once for the tax year 2015. Doing something like a sum of all revenue across all those rows would bring in data that was potentially duplicative across multiple charities. Keep in mind understanding that data model or working with your subject matter experts to determine what those unique row conditions or field should be is critical. Next, if you're combining data historically, be sure to use that table suffix and be as granular as possible so you can avoid reading more data than you intend to use. Lastly, if you've used SQL bunch before you might have run into self joins, consider using those Window functions instead for optimal performance. Another great performance tip is to keep intensive operations like order binds to be the very last thing that you're doing in your queries. For example, here as you can see, we have a sub-query, a named sub-query there with line 12, we're actually ordering it. But we actually don't need to use an ORDER BY there because we're not doing anything with ordered results. Now one of the easiest ways to spot this new queries if you're doing more than one ORDER BY. If you have an ORDER BY like you see here and the named sub-query on line 12. Since we're not actually doing anything with the order of that data there, we can actually remove that ORDER BY all the way to the end if you wanted to do something like ORDER BY charity names. This allows you to take the advantage of any filtering that's going to happen before. We're only ordering by those results that actually matter to us in the very end.

# Prevent hotspots in your data
Next up, let's talk a little bit about data skew or hotspots inside of your data. One of the things that you should be aware of is the frequency of occurrence of particular values in your dataset. One of the examples I like to consider is if you had a million records of customer orders, but 99 percent of them is skewed to one particular customer, just keeping in mind what that's going to do to your workers or your slots as they begin to process your queries. Now, one of the workers instead of BigQuery likes to have all of the values. For example, if you're performing a some of the customers order value over time, one of the workers actually prefers to have all of those values for customer 1, which in our case represents 99 percent of the data. Whereas all the other workers will be waiting for that particular slot to finish performing this calculation before they can get access and combine those results together to display for you. One of the ways you can uncover this is by using the Query Explanation map. You're going to see in a minute with a high delta or difference between the max time for one of those workers to complete the task versus the average time of all the other workers. Now behind the scenes, BigQuery will automatically attempt to reshuffle that workload among different workers. But one of the things that you can do even before it gets to the workers is filter your dataset as early as possible.

# Diagnose performance issues
We mentioned it a few times, but let's talk a little bit about the main tool in your toolkit when it comes to diagnosing performance, and that is the Query Explanation map. Let's talk a little bit about what you're going to see here. It's a lot of horizontal bars, and this represents the amount of time that each of your workers is spending in each of those key stages. Now because your data and your query is massively mapped and distributed, you could have multiple stages as your workers communicate between each other to get the job done. Now you can see how many rows are being processed in each stage, all the way on the right. The top shows you the input and then the lower right ultimately shows your output, which is the results of your query. As we mentioned before, having a high cardinality or high distinct number of Wikipedia editors in this particular case makes it not the smartest query to use for a large GROUP BY. The main takeaway here is the grouping by something that has many distinct values equates to many force shuffles or communication, you can think of it as communication, between each of your different workers. They're also called shards or slots, many different names for the workers that actually perform the work in your query. That's because again, the key thing to remember is that each individual worker and sometimes you can have upwards of 2,000 workers working on a query at once, is selfish and likes to have a partition of your data where it has all of the IDs on its own local memory to perform those aggregation operations or any other querying operations across that subset of your data very efficiently. Two more concepts we want to cover. One for performance reasons. We talked a little bit about this when we talked about performance remedies for the Wikipedia example that we solved by saying, Hey, if you wanted to potentially just look at one particular year, like last year, you could actually save that data table in a separate permanent table. Similar to that is the concept of table sharding. Now what table sharding does is if you have a large data table but you are only you're using a certain section of it over and over again, you shouldn't have to pay the costs performance-wise of scanning every single records. All the way on the left, what you see is traditionally in relational database systems, you'd have a database administrator or DBA that would take a look at your database and then manually partition or pre-allocate that space in very defined partitions on disk. You'd bet pretty apt to VM a hardware master to deal with that, to get those individual partitions set up and perform it along with your queries. Second later on down the road, what actually came out is you can manually shard your own tables and typically as you saw with the GCOD, whether examples that we've covered in the joints and unions, you'll see a year prefix on the end of it. With that one, you saw GCOD in 1929 all the way up until today's a data table. It results in many different tables but you're not scanning all of those table records if you just wanted to query last year's temperature data. Now one caveat I'll introduce here for that middle section where the sharding the tables is that there is a what I'll call a transaction cost or an overhead cost that you're going to incur with creating all those different tables and that's because there's metadata associated with each of those different tables that BigQuery itself needs to store. The last and best way that if you came across this issue and you just really want to query most recent data like 2016, 2017, 2018, is to actually set up a special version of your table, which is called a date partition table. It can be just like a normal table, except that it actually has what's called a partition column in it and this is just like a date time field, as you see here all the way on the right. The example that I'm going to go with is if you have a massive amount of customer orders historically going back, say 30 years, but you really only interested in querying things from like the last three years, you want to wait a partition off the table in and of itself so that you're not scanning over everything, as is typically the case when you're just doing a select query. What you can do is you can actually predefine your table when you create it not only the caveat here is that when you actually create the table, it has to be setup this way as a partition table. Then you can specify a date column there as an automatic partitioning field where we can partition by day, partition by month, partition by year. Let's take a look at what that query would look like if you're querying from a partition table.
Play video starting at :4:21 and follow transcript4:21
Going with that third best-case example, where you have a single table, so you don't have to worry about doing an union wildcards or anything like that and you have a partitioning column setup there on each of those different days. How you would query that is through one of the reserved fields as you see there in the where clause and the left called partitioned time. Then we can do there is setup between a timestamp of such and such and then another different endpoint there and then what that'll do is the query will go on and it won't scan through every single record you have in your massive table, it'll just only look at those partitions. This is one of those unique cases where we talked about earlier where your where clause will scan every single record in the table. This is one of those caveats where that's actually not actually the case. When you have a partitioned table automatically set up by date partitions, then the where clause can actually filter out individual partitions before finding the results of your select statement. That's pretty cool concept. Last up to cover, if you really interested in performance, all of the BigQuery logs are actually outputted the Google Cloud Storage and then read into Stackdriver. Stackdriver is the performance monitoring tool, not just for BigQuery logs, but also for all other Google Cloud products. You can actually export all of your BigQuery logs directly into Stackdriver and take a look at things like your worker or slot utilization, how many queries do you have in-flight? How much bytes you've actually uploaded? How much you're storing up persistent disk? If you took that to the next level on this next slide, you'll actually see using Stackdriver logs, you can get out individual queries that are performing poorly potentially, or queries that are costing your organization the most when looking at that pricing model there and you can perform what I like to call the BigQuery audit. Now, one of the big tips I have for you if you're setting up your Google Cloud project for the first time, be sure to actually go into billing and as the second bullet point there says, you can actually tick the box that says export your billing logs into BigQuery. Now that'll actually show you all of your auditing and all of your billing history directly queryable in BigQuery, much like you'd like to run some SQL analysis on it and then you'll be able to build Google Data Studio dashboards like the one that you see here on the left. It's time for a recap. Performance, since you're paying for the total amount of bytes that you're processing, you want our first limit the amount of columns that you're returning and also consider filtering out your rows by using that where filter it whenever you can. Now this doesn't mean you can't write queries that process your entire dataset, BigQuery of course, was built for petabytes scale, you just want to be mindful of the resources that you are consuming. We then covered the Query Explanation map, where we get those cool visual cues of the types of work that is most demanded by your queries. Note here that a large difference between the average and the max could indicate a heavy data skew, which can be helped by filtering your data as early as possible. Now we've covered some SQL bad behavior, like selecting every record and ordering it without using a limit clause. Finally, if you're only accessing a recent data, like the most recent events, consider using table partitioning to reduce the total amount of bytes that are scanned by your query. Now let's troubleshoot some of these next queries in this next lab.