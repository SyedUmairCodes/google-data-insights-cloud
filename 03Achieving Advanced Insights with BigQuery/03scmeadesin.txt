# BigQuery versus traditional relational data architecture
Now ahead is one of the most critical modules to pay attention to. Even if you're a sequel guru, here we're going to look back at the evolution of modern databases and end with how the technologies behind big query addressed some of the limitations in architecture that prevented true petabyte scale data analysis.
Play video starting at ::17 and follow transcript0:17
We'll then discuss a core database concept called normalization and end with some pretty cool data structures like having nested records inside the table.
Play video starting at ::26 and follow transcript0:26
Now, let's start with the Database Evolution Journey first.
Play video starting at ::30 and follow transcript0:30
Okay, let's dive right in. So a lot of these concepts, we're going to step away from a lot of the sequel syntax that we've learned before and we're going to think more about database theory and the evolution of databases over time. And the example I'm going to walk through is our IRS data example that we've been using for all of this course. And let's take a look at this schema in the way that it's actually storing these expenses. So we have over 20 expenses that are recorded for each of these charities. You could have legal expenses on their form for 2015 that they're filing, accounting expenses, lobbying expenses, royalties, travel expenses grants. All of these data fields need to be stored in some kind of structured row and column format, right? So let's take a look at how we actually have done it. We being the IRS and thus suggesting that data into big query. So that form data is actually pivoted in a way, right? Where instead of being a list of expenses, this is just an increasingly wide table of all these different fields and this if you query the irs_990_2015 table this is exactly what you would see for each record. You're going to have one EIN and a whole list of all the fields that are available that have been filed on that 990 return with all the different field names. And these are some of the examples for the expenses, right? You can see legal fees, accounting fees, fees for lobbying, office expenses. Now, here's the problem with structuring the data this way. What if I asked you for 2016, there's ten new types of legal expenses that I want you to add to the IRS form 990 for charities to be able to fill out. What would that mean for your your data architects, right? So if the PDF form that charities have to fill out has 10 more fields. No problem for them. But you get that new form in. What about all this old data that you have? What about the schema that you already have, it's going to make fundamental changes, right? So you can technically go wider and create new columns. But think of the downsides to that anytime you're doing unions on the past, you're going to have tables that have mismatched columns. That's the least of your worries. The biggest part is when does it end, right? Maybe some years lose expense columns. So you're shortening the schema and then maybe some years have many more columns and you're just widening it. So, constant changes to the schema is bad design and we need to find a better way to cope with flexible amount of fields we want to store and the answer is potentially not having them stored as fields at all.
Play video starting at :3:20 and follow transcript3:20
So as we cover, that's not really, that's scalable in either direction. You don't want to make continuous changes to your scheme over time. You want to be flexible. So in the traditional relational database world, let's talk about what one of the potential solutions was.
Play video starting at :3:37 and follow transcript3:37
So what you can do is you can actually break apart your expenses into a completely separate table. So instead of storing everything in one massive, what I call the mother of all tables, you can actually begin to break apart. Those fields as grows in another table and tables are much more flexible vertically, right? By adding and appending new roads. Then they are horizontally by continuously adding and removing fields. That's no fun. So let's cover this example. So here we've broken it apart into three separate tables instead of one massive table that has all the different field values. You have the organizational details and one and that's the item in blue and then in green you have the historical transactions. And you can actually look up those expense codes right? From your expense code look up table and this basically says expense code one is lobbying to as legal, three's insurance and maybe 50 could be travel or something like that. You can have as many items in this look up table as you want. And then the historical transactions can just look up against that. You might be wondering why have an expense code at all. And then this gets to another point of database theory, which is try to have one fact in one place. So if you end up renaming something lobbying and renaming the word lobbying to lobbying services, if you had that repeated tens of thousands of times in that transactions table because you didn't have it broken out into a look up table, that's right for error as well. So generally you just want to have as much separation as you can. But again, we'll talk about when, how far is too far. And this entire process that we cover is called normalization. So normalization is in the relational database world. And this is again apart from big query and apart from any specific technology, this is just database theory is breaking apart, all of your massive data tables, you're centralized consolidated data tables into these separate but related tables. And this is the foundation of the relational database model, right? So what happens if we go too far and when you talk about that in normalization versus de normalization. So breaking apart, but then potentially coming back together, right? Now let's talk about some of those trade offs.
Play video starting at :5:57 and follow transcript5:57
So what's the benefit? So we cover this, right? So the organizational details table can now grow if you have new charities that are joining every year or removing every year boom, that's fine. Just grow as much as you want and the transactions you have expense codes or you have any kind of code, it might not even necessarily be an expense, right? And then you can look up against an ever changing list of ex expense codes that you see there. So you have much more flexibility and you don't need to grow horizontally and change that fundamental schema, which you absolutely want to avoid at all costs. What's the downside? It's a drawback is if you want to do a consolidated query, like give me the lobbying expenses for the New York association company. Now in order to tie those insights back together as you've learned how to do in sequel with those joints and unions and merging the data together, joins are now a necessity as part of the relational database model, right? And joins for performance reasons, joining data across multiple different tables and then doing that look up is a big performance hit. But again if you're relating database tables together across different and disparate data sources, you're going to have to use joints, right? There's no other way out and for big quarry, we're going to cover a particular answer to this relational database problem. Okay, so we now have data pulled from three different sources. Not ideal when it comes to performance, but for a squeaky squeaky clean database architecture design. This is what we want in theory. So what can we do? We can actually say, hey, well you mentioned that having a separate look up table for individual expenses that just maps this arbitrary code. The expense type is a good idea. I don't want to do that. I don't want to have a third table. I'm going to delete that third table and instead, I'm just going to actually have the expense code be spelled out for what it is. And you trade a little bit of you included some creeping redundancy into your potentially for duplicative fields.
Play video starting at :8:3 and follow transcript8:03
And what you've done here is potentially you've traded redundancy, you've added a little bit of redundancy, which is potentially bad in this case, right? You could ever repeating expense goes like lobbying, lobbying, lobbying. What happens if you wanted to change lobbying to lobbying services? Again you have to change that for every road there in favor of a performance boon because you're no longer joining across three tables. You're joining across just those two.
Play video starting at :8:29 and follow transcript8:29
Okay, here's another issue growing a relational database at scale vertically does have its limits. So, traditionally, if you're trying to scan across millions or billions of these charities, right? If you're doing a secret scan, that's traditionally very hard and computational expensive when you have things in the order of billions. So having one massive table, think of this like a spreadsheet, you have a massive vertical scroll bar because you're just trying to capture all that information in one singular table, which does have its benefit, it means that all facts are in one place. But if you're talking on the billion road level, it's not exactly efficient to begin sorting and scanning through a lot of this, massive vertical tables. So, in the relational again, it's not big quarry in the relational database realm. If you've probably, if you're a sequel guru, you've heard of these before, but indexes these pre sorted indexes, which are kind of like a separate artifact were created. So for common queries, right? If you wanted to get the rank order alphabetically of all these companies and you can imagine this at scale, I'd say there's 10 billion records here or however many. And that's a very, very common query. So it optimizes read performance because you can actually read from this index much, much faster because it's pre sorted because. Hey, we're getting a lot of folks that want to do this normal ranking of these company names, we're going to create this index. So we can actually reference and have that be much faster. So you're trading read performance, read meaning, select company name from this at the expense of write performance. So if you had a changing list of company names, you'll have to update that in two different places now. So if New York association Inc was no more, you have to delete that from both the organizational details table and your index table would have to be updated as well. So there's a trade off there.
Play video starting at :10:27 and follow transcript10:27
So for big query this, both the concept of scaling out into multiple different tables and the idea of building these indexes to handle massively, massive amounts of rows and tables is fundamentally handled a completely different way. And we'll talk about that next.

# Denormalized, Column-Based Storage
BigQuery architecture, this is the best part of the lecture. What makes BigQuery special? It's these three key innovations. This is going to be really fun to dive into, even if you're a SQL guru because you get to look at the underpinnings of what makes BigQuery, BigQuery. First and foremost, columns in records in data are actually stored as individual compressed columns inside of what again, I like to call them massive hard drive in the Cloud, which is Google classes. That's the latest massive scalable storage layer, and we'll go into combat storage and what that means in just a second. Second, the actual table itself, it's not one skyscraper tall single table. That massive table, billions and billions of rows is actually broken up and sharded into a bunch of small little pieces, which are then called upon to execute units of work in a massively parallel fashion, which we're going to cover as part of that section. Then third, an answer to the first part that I brought up, where joins are apparently a necessity in the relational model. Not so with BigQuery because you can actually store these parent-child relationships directly within the same table. I like to think of this as if you can imagine for IRS example. If you had organizational details and you wanted to drill into, remember that plus sign on some of those things that you've seen where you want to drill into details, drill from the parent and expand the child details, drilling into all of Kaiser's filings for 2012, 2013, 2014, 2015. You can actually do that and store that architecturally in a concept that's called nested in repeated rows. These are three massive topics, and we're going to approach all of them in detail. You're going to get a nice lab after this practicing all of the SQL mainly as that third part for the syntax and nested repeated fields. Who's ready to get started? First and foremost, BigQuery is column-based. What does it actually look like? As you can imagine, if it's a column-based, we're storing individual columns. Now, what does that mean as far as performance from reading a lot of these data values from disk? In traditional databases, you're storing individual records. If you have a very wide record, take our IRS, the tax filing example where you have like 200 columns. The way that's stored out traditionally on disk is each individual record is stored as an individual record out on disk. When you query that record, you're pulling up all of the bytes of all of the different columns that are present. Even if you just wanted to get 3 out of the 200 columns, that record itself can't be broken apart. Realizing that Google pioneered the way for column-oriented storage in that Dremel white paper that we talked about back in 2010, and is continuously innovating in how to actually have new compression methodologies for storing these columns in the smallest file size and disk and getting at the absolute fastest speed. Now, what does that mean for you? At the end of the day, selecting only a few columns means you're only actually getting the performance hit for how big those individual columns are. Back in our example we used e-commerce products. If there's a column that has product reviews, and the product reviews each field has a paragraph of text, lots of bytes. If you're not selecting that column, and you're just selecting the customer who ordered it and how much they paid, you're just returning those two particular columns. It's a mindset shift in thinking. Instead of record, it's actually being stored as an individual column but on disk. One of the final points that we'll cover here is with the column focus, you can actually be storing those related values. In SQL, you're going to be selecting those individual columns, and that's going to be very fast to pull all of those values because they're actually stored that way vertically, as all those different column fields, as opposed to individual records that you have to access all of the record content, and then combine multiple of those records at the same time as you're producing your output for the user. Column-oriented storage is a key concept.

# Table Sharding
Next, bringing apart tables into pieces, this gets at one of the really fun performance topics, which we're going to dive right into right now. Going back to our example, instead of using indexes, there's no indexes in BigQuery, instead it's handled a completely different way. No matter how big your table gets, it's broken apart, so say it's just like petabytes, 10 petabytes, 50 petabytes, it's broken apart into manageable chunks like say 100 megabytes or whatever some small chunk is. In each of those different pieces are stored in what we call a different shard in the latest rendition of the Google File System, which is called colossus. Breaking apart that data into different manageable chunks is part 1 in a two-part series that is going to be getting your data to your actual query that you want to run, so we're still missing one critical piece, which we're going to go into right now. Here is the BigQuery execution engine network, so if you're really interested in how BigQuery works and the architecture, this in the next two slides, the one to really pay attention to. Dremel, which is the query engine that BigQuery currently runs on, is responsible for taking your SQL query and breaking it apart into a tree of work, basically saying, you want to select company name and order it by company name or very, very complex query, much more complex than this. These are all the different pieces of data and needs, so if you're doing a bunch of joins is from these different tables, if you're doing these calculations, it's going to take that into account and drove all the query engine behind BigQuery will project out this query and basically say, guys, we have this much amount of work that we need to perform on all of these small data shards that you saw on the previous slide. When it's able to do then is basically act as a master, maestro or orchestrator of all this work to basically say, take this little piece of work, I'm going to assign it to this particular worker, or it's also called a slot and basically say, you're responsible for performing this one small task. It's going to do that about thousands of times, depending upon how large scale the query is and how much data we need to process in parallel. Then those individual workers in parallel in concert with each other, then reach out to those shards that we saw earlier, flipping back to the previous slide, and then grab unit of work, some of that data from that charted table, process it, and then they're not done yet. If there's some additional data that you needed from one of your other friendly workers, it's passed between each of the workers in what we're going to call a shuffling of data or a shuffle step. Then additional processing is performed in the data, and that process repeats itself massively in parallel until you ultimately get to the end result and we'll walk through an example of that in just a few minutes. But hang with it, I think the next visual is going to be a little bit more clarifying, not this massive COG one, but the one that comes after. Here you have up to 2,000 individual workers that can take a piece of that query and it process it massively in parallel and reach out and gather that piece of data. There are a bunch of different minions that basically say, this data pieces mine, I'm going to perform an operation on it and if I need data from you, we're going to work together in concert and shuffled together and continuously process it until our work is done. I'll introduce the concept of fairness model for the amount of workers, so if you have one Google Cloud Platform project and you have 50 people using it, and you have one person that has a massive query that's using 1,000 workers, there is a fairness model of splitting apart the 2,000 workers evenly among all the different queries as well and you can again, you can set up custom quotas for each of your different user groups as well. But the key unit or the worker behind the scenes, is that work are also sometimes called that slog, if you remember that reserved slot lecture that we covered way early on pricing. Here's the slide that talks to the performance pieces of what's going on behind the scenes, so we're going to read this from left to right. We're starting out with some data that comes in, so workers are consuming these data values, so you can imagine all the way on the left where it says shuffle and minus one, there is some data that's being consumed by these workers, or potentially the output of other workers that have gone on before these and it's all done massively in memory. Basically each of these workers behind the scenes is actually a virtual machine and you're tapping into Google datacenters, which have absolute massive scale of CPU power and persistent disk and all of this behind the scenes. What these workers do is they work on a little piece of the puzzle locally and if they are able to process their massive chunk of data within RAM, that's great. Worst-case scenario, they actually have to write it out potentially due to persistent disk before sharing it with other workers. But let's go this step-by-step. Step 1, workers and just data do some querying work, they're doing it in parallel, they're doing it in memory, which means it's much faster. After they're done with, say, a piece of the query, like performing a group by or doing a calculated field, then they're going to share their homework effectively, their pieces of work. Now if there's any other additional work that needs to get done, like aggregating things together and say everyone did the calculation and now there's a sum and those are our group I. Then it's passed on yet again to additional workers as part of step 3 and this process continues for however many steps are as needed. Now what you can't see here is that number 1 and number 3 skill vertically, massively, so if you have a crazy large query automatically behind the scenes, the journal querying Engine or the BigQuery service will recruit as many workers up to 2,000 again that are needed and that represents just a cohort of CPUs that are being thrown against your query to process.
Play video starting at :6:19 and follow transcript6:19
Continuing on and the whole point here is that this shuffling of data between these workers and each worker processing just a piece of that data for that query enables massive scalability, so instead of scaling your infrastructure, if you had to do this yourself, you're just buying up all the CPU power and then you have to maintain just a massive static amount of servers. These workers are what we like to call ephemeral, so if you need to 2,000 workers for 10 seconds, 20 seconds, if you remember back for the very first lecture there you've tapped into Google's massive power and as soon as your queries done, those workers are released, everything ramps down and they help your friend who's processing the next query. Think about that and when you think of all of the many thousands of users that are using BigQuery at the same time, that's shared pool of massive, massive amounts of workers because it's again, google is using the same thing for their datacenters to process things like Gmail or search or adds logs as well spilt on the same platform.

# Introducing Nested and Repeated Fields
Now that you've got a hint at BigQuery architecture behind the scenes. Let's cover a little bit more on the schema discussion where we talked about possibly the fact that joins are not a necessity and potentially we can actually store parent-child related data, almost like two tables in one, through the concept of nested fuels. This is where your SQL knowledge is going to peak with a very advanced concepts that we're going to have here. Let's dive right in. This is the last part of my favorite lecture to go over. Put your seat belts on, let's just jump right into it.
Play video starting at ::41 and follow transcript0:41
Here we go. BigQuery architecture introduces the concept of repeated fields. If you haven't seen repeated fields before, it's a magical concept. Where we had normalized data before, all the way on the left, we have two different tables. You have people and then the cities that they lived in, again, keeping one fact in one place and having a common key to link them. That's a traditional relationship model of databases. Again, you're sacrificing performance because you necessitate the use of joins. But it's better for, what we like to call, better BETA Schemas for those relational databases. In the middle, you have the opposite. If you want to optimize for reads, but potentially just have redundancy in the amount of data that you're storing, that's a denormalized view. We call this the mother of all tables. Then the third, the new one you've probably seen before is you see the name, the age, the gender, the cities lived, and something strange that says repeated. Then this is the first time you might have seen a schema that actually has a tab in it, where cities and years lived seems to be directly underneath where that cities lived is and that's exactly because that is a nested child field within that schema. That concept of repeating these rows is exactly what we're going to spend the next pieces of these slides going over. Review relational model requires expensive joins. Now here's the real crux. BigQuery can actually use the nested schemas to get at that high-scale. Imagine if you had all your data in one table and you didn't have to do a join, it's much faster to refer to it that way. This might look like I have purposefully obfuscated some rows here with that gray box, but that's actually not the case. That is actually what your query results will look like inside of BigQuery. For this company name, New York Association Inc, those three expenses, lobbying, legal, and insurance, are actually child nested records underneath that singular record. Where that vertical blue bar is, all of that is one row. I'm only showing you two rows here. The first row is New York Association Inc. and the second row is ACME Co. It's not four individual rows. That's because you have this collapsible nesting of these child rows into that first one. We'll walk through a few more examples and see how that actually looks like inside of BigQuery, but just conceptually, work that around in your head.
Play video starting at :3:9 and follow transcript3:09
Let's talk about some of the performance benefits. Again, you're not joining across two different tables because you almost have an implicit built-in join because of the way this is actually set up. The great news about this is, that gray space right there, if you're doing something a select, distinct on the company names, you're actually not iterating through four rows. Again here you're iterating just through those two. It's like having your cake and eating it too. You can get all the performance benefits of just counting quickly the parent records. Think of an orders table with nested order details inside of it. If you want to get the orders really quickly, you can get that benefit, or if you want to expand an individual order and look at some of the items that are in there, say, it's a 1000 items, you can unpack or do what's called UNNEST those rows and access those as well.

# Introducing arrays and structs
There's those two data types that I alluded to way early on in this specialization that may be new to a lot of you called arrays and structs. These are general concepts in SQL, even outside of BigQuery and they're natively supported on the basis of repeated fields as we get into, and we'll practice that a lot. If you are a veteran to BigQuery, there is actually a different way that standard SQL handles what's called the flattening of these arrays as opposed to legacy SQL. We'll cover that very briefly. Last but not least, we'll get a lot of SQL practice within these repeated fields as well. Let's jump right in. First and foremost, let's introduce what arrays and structs are. We'll start very simply. If you haven't seen arrays before, it is a data type and it is an ordered list of zero or many data values but the key caveat is, they often have the same type. Here if you wanted an array of fruits, you could have your raspberry, your blackberry, your strawberry, and your cherry. Almost got all berries there. How that actually is stored. An array is stored within those brackets and each individual element of the array is stored with a comma in between them. That's all just one field value, that's actually what could be returned. What that actually looks like, if you're going to select that inside of SQL. If you actually ran that query on the left, you can create an array yourself. You create it with those brackets and you say, hey, select this field and in this field, have it be this array, raspberries, blackberries, strawberries, and cherries. Look at what the output that you get is. Again, it's that one long row, so this isn't four rows, so it's actually four pieces of data in one. Instead of displaying the results as raspberry, blackberry, strawberry, cherry, and bracket all in one row, BigQuery naturally flattens that out into what looks like four separate rows, but is in fact what we call nested rows. As part of BigQuery standards SQL, it'll naturally flatten those rows output for you. We're going to start with some of the basics with these arrays. We're going to keep building on these concepts and why this is such a big deal when it comes to having this concept of nested rows. We've got arrays down pad, it might look strange again like that row is very large vertically, but just keep in mind you can actually have what seemingly is multiple data values within one row through the use of an array.
Play video starting at :2:30 and follow transcript2:30
Some cool operations that you can do on the size of the array. For example, if you wanted to get the number of items in your grocery cart. If those four were your fruit shopping for the weekend, you could get a special array function in there called array length, and you can get the total size of that array. Here again, we're just using a quick common table expression just to get some of that data in there, we're not pulling it from any particular table. You can run these queries yourself and be [inaudible]. There's particular array functions for accessing the contents of the array, and you can manipulate each of the different elements in the array. There's a lot of different functions that you can imagine you can do on top of arrays, sorting or filtering, which we're going to cover in a few slides. As we covered a little bit before, BigQuery will implicitly flatten out your arrays. What I mean here is, if we had Jacob's order in the grocery store, and it was an apple, a plum, and a pear, instead of having that comma separate it, like you saw that's how you created the array in the left hand side. Instead of having apple comma pear comma plum in the output, BigQuery, what essentially is pivoting it, which we call flattening it. You'll see again that suspicious looking gray space beneath Jacob, because Jacob is essentially that parent record where you have a lot of those child records, the items that he has associated with that. That again, is all stored within one table. Whereas normally this would be split off into an orders table, or customers table, or a details table. Let's keep moving along.
Play video starting at :4:9 and follow transcript4:09
How do you access elements in that array? In order to do that, we need to use the function that's actually called UNNEST. It basically says, well, I need to do some work on this array, if I'm going to join it, or if I'm going to filter on it. Basically, what that's doing is it's saying, Jacob, which of these items are associated with you? You've got the apple, the pear, and the peach. Let's UNNEST all of those items and then get them out on three individual rows. Instead of having everything in one row, you now have three separate rows where they all, each individually belong to Jacob. That allows you to perform those operations like filtering. If you wanted to filter for just a pear or a peach, that's where you're using your where clause now because that filter is on rows, because you now have three rows instead of one. The syntax for that is on the left, you are selecting the items, the customer name, and then you're UNNESTING or unpacking that array there, and you actually use UNNEST in conjunction with that cross join. If you remember, I've probably scared you a lot with the unintentional CROSS JOINS. A CROSS JOIN is just an artifact that we need to use here to pretty much copy Jacob's name in additional two times to get it associated with all the items that he's ordered. Going back to the previous slide very quickly, you'll see it's just Jacob and then two blank spaces beneath that essentially, but again, keep in mind it's conceptually just one row. Here, we have Jacob, Jacob, Jacob and that's in the effect of, because we broke open that array into three separate rows again, and we needed to break open that customer name to match and that's why that CROSS JOIN is there. Let's keep on diving down into the rabbit hole of arrays. Some useful things that you can do if you wanted to pack everything back up. If you want to do, create an array from a field that you already have. Say you have the dataset on top, apples, pears, and bananas. If you wanted all pack that all onto a fruit basket, you could use something that's called an array aggregator or array aggregation function. What you do there is just pass in that field and boom, those elements come into an array itself. If you have data tables that you eventually want to get into what you're going to see in a minute this parent-child nested structure, the first step is potentially aggregating those elements into an array. One row instead of three.
Play video starting at :6:40 and follow transcript6:40
Another nifty function that you can do, much like you saw in the advanced functions example, where we're sorting those partitions, you can sort the elements in an array, but just by ordering it inside of that function. So you have an array aggregation function and you'd basically say, all right, for this new array that I've just created, ordered alphabetically so apple, banana, and pear.
Play video starting at :7:5 and follow transcript7:05
Okay, so let's get a little bit more complex. We have three shopping lists up top. Person one is apple, pears, and bananas, person two has a carrot and an apple, and person three is just ordering water and wine. Now we want to find all the different shopping lists or shopping carts where there is an apple present. In order to do that, what we need to do first is set up our data as three separate arrays. That's how we got that data up top, and that's that first query. It's just a simple with Steven, that's just says my data is three arrays union together. There's nothing special about that one. Now on the bottom here, we're specifying, unpack each of the different grocery lists. Again, we have to unpack them by using the UNNEST. And that allows us to use these operators and filters as you see with that where-clause, and how you filter in a particular array is by it's a little bit counter-intuitive. You have where apple is in that particular list. Whereas you might have expected like where the list contains apple or something like that, you actually have what you're searching for as first. Then at the end of the day we pack it all up in array as it was before. So, you open the box and look at all the items that are in the box. Again, that's by using the UNNEST function there. Then we pack it all backup by using the array. And that's how you get your filter performed. Now consider the two, the results there. You have apple, pear, banana, and carrot, apple, both of those arrays themselves are returned because they each contain apples. But you'll also see that the third array is returned, but it returned with an empty set or a null. That's just one of the natures of the arrays where it did not match.
Play video starting at :9:7 and follow transcript9:07
The next big concept, so this is a completely separate datatype from arrays, but conceptually a little bit similar. It's a struct or a structure. Structs are absolutely flexible containers that, unlike arrays, you can actually store multiple different data types inside of that. Not just all string values or not just all numbers. Well, that's the limitation of an array. Inside of a struct, as you'll see in this next example, you can actually store completely different data values. Now it starts to get a little bit strange. Inside this one data field, we're generating these multiple columns. This is interesting. Let's look at the two different struct values here. We have an age value and a person's name. We specified two different values. We'll say 35 is an integer and Jacob there is a string, and both of these are actually stored as a single struct. This is just a singular structure. Of course, you take a look at the results that you have. And you have this weird column placeholder. You have agent name as you see it there. But you see this weird prefix there. What is that? That's again if you've seen that before is because we didn't give it an alias. Struct is the container that you're creating. It's almost like a table. It's a table within your SQL query. You'd give it a name and now you have something that looks like almost a drill-down capability. You have customers dot age, customers dot name, and you've got that all within that singular field. That customers struct generated both of those values for you. It looks and behaves very similar to a table. You'll see when we start mixing together arrays and structs that the real magic is going to happen.
Play video starting at :11:6 and follow transcript11:06
Okay, so here is the mind-blowing moment. As we mentioned, a struct can have many different types of data inside of him. An array is a data type. You can actually have an array as part of a struct. Walking through our grocery store example, you have Jacob as a customer. He's 35 years old. These are the items that are in the shopping cart as the array. Now keep in mind the limitation in the array is that all the items in that particular field have to be the same datatype, not true of the other values in this struct like age and name. Again, age is integer, name is a string value there. But looking at, we're building what looks like a table with just these flexible containers. We have these repeated fields right in the array. Now you have an array nested in a struct. Let's get even weirder. You can actually have the other way around. You can have a gigantic parent array that can contain multiple structs as values. This is where it gets really weird. Let's break it down piece by piece. Again, you create a stroke with those brackets. You're selecting brackets, a struct. You have an array of structs, and that first struct is Jacob is 35, and then here are his items. Let's walk this through. His items are in array of items inside of another parent struct. Let that process. As the second item in that parent array. By the way, that parent array is just called customers. We have another customer, and inside that customer is a struct. We have Miranda 33, and then here are the items that she ordered as part of her array. Now a couple of interesting pieces to note. Just as in the example, we happen to have the same number of items in the array, but you could have a very differing amount. You've got water, pineapple, ice cream, and soda. You could have five or six or a 100 items for Miranda and just three items for Jacob, the child records can grow irrespective of what the other customer ordered as you might expect. You're beginning the pieces together in your head. You might even be able to start looking at what are some of the advantages of having this repeated structure of having multiple detail records alongside lookup information, but not having that lookup information and repeated unless you need it. Let's go one more layer deep. Now that we have this construct that you saw before, we just created it. Let's go back to review what we just created. We created this customer's shopping cart list combination. Now what we want to do is we want to filter for only customers who have bought ice cream. We don't need to do any joins to do it. That same code that you saw before that generated the customers in their orders is copied over to this slide and we just dumped it inside of a common table expression called orders. Orders was from the previous slide, and now we actually want to operate over it. As you saw with Apple filtering example is the exact same here. We're saying, give me the customers, and the customers again is a struct. You're just selecting what seems like one field down below there giving the customers, and that'll ultimately return those three fields. Because customers is a struct and it has "customers.age" "customers.name", "customers.item", that's the power of strokes. Giving the customers from the orders, and again, orders is what we created before. What I want you to do is apply this filter to only get the customers that have ordered ice cream. The syntax may look a little tricky here, but just know that this is just part of the deal. In order to unpack the elements in the array, we needed to do two things. To unpack the elements in the customer's array, we first need to "Unnest", meaning who are all the customers were looking at. Let's, "unnest" that first array and say, now we've got Jacob and Miranda, we move on "nested" the customers. We also need a cross join that again. You can imagine on the right-hand side you have Miranda near the water, Miranda against the pineapple and Miranda on the ice cream. That is that cross join effect of splitting apart a single row and three separate rows before we pack it all together, and that's what the cross join does there. Then last but not least, we've already unpacked individual customers. But within customers there's yet that secondary nested array which is the items. We need to do another "unnest", and that's where we actually apply our filter. All the way at the bottom we say, "finally, I've unpacked and I can examine the customers, and now within the customers, they have their own shopping lists. Now I want to see where ice cream is in the customer's items list." Again, to look at that items list, we have to unnest it first. When you actually execute this entire query, that's when you get the result all the way on the right. Again, think of the real power here again is that query or just selecting customers, your selecting what is technically just one field. It's just that field happens to be repeated because that stroke in it of itself has multiple different values. Again, the crazy part is one of those values happens to be an array which has nested items inside of it. That's the big mine explosion moments. Let's do some recaps that's as far as we're going to go. Here's the big moment. Nested (repeated) records are arrays of structs. We just covered this because arrays, so you can think of arrays of structs as walked back through the previous example to just to really solidify this, this is the critical point of this whole lecture. An array of struct. The "array of customers" means you can have more than one customer. You've got Jacob and Miranda. You have within the structs themselves, you could have multiple different data types. You can have integers, you can have strings. You can have the arrays, which then gives you the ability to have those nested fields that we can talk about and unpack later. But that's the key message here. Those nested (repeated) records are just an array of similar values. Those values happen to be structs and structure extremely flexible because it's almost like you can store a table inside of those trucks, very interesting concepts, and you contrast that. This looks very similar to doing a join on two different tables. But as you've seen that second bullet point, that join relationship is implicitly built into the table. You have both the customer, which is generally just like in a separate table, and the details of what they ordered in the same table. That's extremely powerful when it comes to performance characteristics. Of course, as you saw in SQL syntax, you have to unpack the arrays using the Unnest.

# Flattening arrays: Legacy versus Standard
These next time topics we're going to go through them pretty quickly. If you're curious, if you're inheriting a lot of legacy SQL scripts, you actually have to explicitly FLATTEN your arrays. Imagine again, back to the first example of the raspberries, strawberries with the commas. It doesn't necessarily in legacy SQL pivot those into those three conceptually separate rows that are a part of that one row, so you have to actually explicitly FLATTEN that. But within standard SQL on the right the flattening happens implicitly, and then you need to actually access those array elements with a CROSS JOIN and the UNNEST. If you see some of these functions on the left like WITHIN RECORD or doing the nesting of those records in an array, that is the old legacy SQL syntax. In the new standards, you're going to be using array operators. You're going to create those nested rows by doing something like creating an array through an ARRAY_AGGREGATION, or operating over the array using ARRAY_LENGTH to get the number of items in the shopping cart or something like that. This is more of a heads-up, in case you see something like this come across your path.