# Introduction to the Google Analytics ecommerce dataset
One of the hardest parts about making a course like this is finding a practice data set that any analyst can relate to and want to query for insights. Now, with the latest edition of this course, we're excited to bring you a real products and e-commerce dataset with over a million site hits in a year of transaction records from Google's own online store, where you can buy Google branded merchandise. These transactions have been recorded through Google Analytics and made available publicly through BigQuery. With the knowledge of BigQuery that you'll practice throughout this course, you'll be able to answer real world e-commerce questions like, which customers have added items to their carts but then abandoned them? How do I create a review of those customers that can nudge them to complete their transactions? How do I set up cohorts or a segment of my customers based on their behavior so I can target each one of them in a more personalized way? Or even what keywords and referring partner sites have led our visitors to our e-commerce site, and what pages did they visit while they were there? My personal favorite for analysis, how close were they to transacting?
Play video starting at ::54 and follow transcript0:54
The great part about working with this dataset, is all the queries and the insights that you derive here are going to be directly applicable to your own Google Analytics datasets. Now, if you're not a fan of e-commerce, the analytical tips and techniques that we'll cover here are sure to be useful in your own queries.

# Common data exploration techniques
This is one of my favorite modules. What we're going to do here is compare common data exploration techniques, and focus on writing some good SQL in BigQuery on our own course dataset.
Play video starting at ::11 and follow transcript0:11
Now that you're a little bit more familiar with the dataset that we're going to be exploring, which is that IRS charity dataset, let's talk about your different options for how you can actually explore that data. So as a data analyst, you're not necessarily limited to just using SQL instead of the BigQuery WEB UI. There's also some other pretty cool data preparation tools like Cloud Dataprep that we're going to introduce you to in later on. And last but not least, you can actually explore your data visually using a visualization tool, like Google Data Studio, or Tableau, or Looker, or another one of those tools. So it's not necessarily a linear process going from writing SQL in Bigquery, processesing it, and then visualizing it. You can use each of these different tools at any point that you want, and a lot of it is a matter of preference, depending upon who you talk to. So the first thing that we're going to explore first is using the SQL approach, and using the BigQuery Web UI. Why? Because SQL is a very good skill to have, almost an imperative skill to have as a data analyst. And it's one of the fastest ways you can interact with data behind the scenes inside of BigQuery. And it's fun.
Play video starting at :1:18 and follow transcript1:18
So what do you actually have to do before you write this query on your dataset? The first is often the hardest part, is thinking up of a question, or some kind of unknown, or anything that just kind of gives you interest about the dataset. So, here you could look for something as simple as, well, just give me the top revenue or the lowest revenue for these organizations in this dataset. But coming up with these really complex questions can often be the the most challenging or difficult part right? It's like a blank canvas when you first look at the dataset, and then you have to determine, well, okay, well, where do we start? And as you see a little bit later on, when we get into Cloud Dataprep, having some basic statistics of frequency of values, and how many meet the data type constraints, and whether or not you have missing values, can be a great first start. But for now, we're going to start with a blank canvas, which is just the dataset schema, and then throw some SQL on it. Second, of course, is accessing that dataset. This presumes that you already have loaded your data into BigQuery. And again, here we're using the public dataset. And last but not least, as you're going get very familiar with in this talk, is writing the SQL that is going to query the fields and rows that you actually want returned as part of your question. And converting it from your question all the way in the left, to interpreting it as part of like SQL is a skill that you're going to master as you get more and more familiar with basic, intermediate, and advanced SQL.
Play video starting at :2:49 and follow transcript2:49
A 2 second background on SQL. So I call it Sequel or SQL. It is that Structured Query Language. It's been around since the eighties. There is a national standard language library for SQL, called ANSI 2011 SQL, and BigQuery Standard SQL follows that, those standards and writing the query in Standard SQL mode gives you a lot of those performance advantages that we'll talk about in later courses. It is pseudo English, so you're using things like SELECT, which basically means give me these columns FROM this dataset, give me the names of these charities, give me the revenue that they have FROM this particular dataset table and then do some kind of manipulation on it. So if we hear ORDER BY, it's synonymous with sorting, sorted alphabetically, sorted highest to lowest. So you get very familiar with these dark blue, all capitalized words called SQL Clauses, as well as the order in which you place them in your queries. And that's what we're going to review for a large part.

# Query basics
Okay now the really fun part. Let's think about how we're going to convert those amazing questions that we just came up with into some best practice SQL. Which is a little bit more rigid than just asking these questions kind of willy nilly. All right. So the first thing, as we mentioned previously, you want to enable that Standard SQL as part of your code, again, it's faster, it's more standards compliant. And if you need a reference in case you've inherited old code that was written in Legacy SQL, there's that reference available to you there, which also includes a migration for which functions now have these new names in Standard SQL.
Play video starting at ::36 and follow transcript0:36
And an additional key point that we want to highlight is, when you're running multiple queries, or say you have a long script because you're just exploring and you want to keep everything all within one document, you can highlight a portion of your code and then execute, Selected or run Selected in just that piece by clicking on that down arrow and then running that selected. Another key difference, and this is what you need to keep in mind when you're pulling data from one of these tables is, you need to escape the table name. And what that means is you need to actually enclose not just the table name, but all the prefixes where it comes from, right? So your table is stored within a dataset like the IRS 990 dataset, and then stored within a project, in this particular case, the public BigQuery project. So we need to actually have project.dataset.table, so a common error here is say, just having just the table. So if you had just a table, BigQuery wouldn't know which table is part of which dataset. Maybe 20 people that are taking this class already just created a table with that name. Whose project am I querying from, because you can actually query data across multiple people's projects. So of course naturally you want to actually get to some table that can actually be resolved to a specific BigQuery table in the backend.
Play video starting at :1:55 and follow transcript1:55
All right, so let's take a look at this query. So this query says, give me, SELECT means give me the columns that follow the SELECT statement, In this particular case top revenue or total revenue. Give me the total revenue from this particular dataset. All right, so keep in mind what we're actually going to be returning here, so total revenue, that's not going to be the total revenue for all the filings in 2015. Total revenues is a field, so in actuality, what you're going to get back is, you're going to get back just the total revenue for every single one of those records, you know, a million of the filings for 2015.
Play video starting at :2:36 and follow transcript2:36
Okay, so maybe we can limit the results to ten so we're not just returning a bunch of data that we have to paginate our way through. But is this useful, especially if you have just the column for total revenue?
Play video starting at :2:49 and follow transcript2:49
Not really, I mean you can see, you can get a general feel that there is a lot of potentially non-zero revenue there. But we don't know who it's from, and I mean in this particular case, it's not even ordered either, so maybe we could potentially add an ORDER BY clause. All right, now we're getting somewhere, right? So now we have revenue, total revenue for 2015 filings, for those exemption filings. Now we're going to order it, and we're going to order it from highest to lowest. And we do that by adding that ORDER BY clause on, near the end, second to last in the query. And we're sorting it from highest to lowest, and how you actually denote that in SQL is descending or DESC is one of the parameters for that value there. It defaults the ORDER BY clause, defaults to ascending or ASC, which would be the opposite, and it seems it works the same way with alphabetically, A to Z or Z to A. Okay, so a key point here is whenever you're ordering something, especially if you have more than 50 rows, make sure you limit it, because there's no reason to return back all that data in those paginated results. You're just wasting your bytes processed, your amount of data that you're consuming.

# Introduction to functions
All right well, going back to the previous slide for just a moment, I can't really tell you if that's 4 billion, 4 trillion, and that would be super useful if we had some commas there.
Play video starting at ::13 and follow transcript0:13
So what we're going to introduce is this quick formatting function. Now, what a function is, is you can take a column like total revenue, parse it into this kind of magical operator called FORMAT. Parse also some other parameters or some other values that are inputs to this function, in this particular case it's a formatting parameter that we have here, and it will create a new result for us. So just adding that gives us a different value. So a key point here is that you parse things into functions and there's some action that's performed on them. In this particular case, we're formatting it to add a comma. What does that actually look like? It looks like this. So, that's 45 billion. So it's much easier to read, but now we have something that's very.
Play video starting at :1:8 and follow transcript1:08
So it's much easier to read. But there may be a couple of caveats that come with that.
Play video starting at :1:14 and follow transcript1:14
So, we're introducing functions, there are many different types of functions. We're covering just the formatting ones to begin with, but again as I like to do, I'm going to introduce you concepts first and then trick you into realizing that it might not be the best way to do things. So here we go. When we applied this function, an interesting piece to know is that, that 45 billion that you see there with the commas is a fundamentally different data value than what you saw before. And a lot of you might argue and you could say like, "Hey, wait a minute, 45 billion, is 45 billion, is 45 billion". But the fact that we performed a calculation or an operation on it means that it actually loses its original identity. And that's exactly what you see there at the column name, you no longer see tot revenue or total revenue, you see f0_, which is just a BigQuery placeholder. So what we need to do is give it a new name. And you give a column a new name by using the AS keyword, and that
Play video starting at :2:20 and follow transcript2:20
renames it, and this is called using an alias. So tot revenue, say that wasn't the most readable, we're just going to say, "Hey, that's revenue." So, select total revenue, and then rename it as revenue.
Play video starting at :2:32 and follow transcript2:32
So, here's the key pitfall. If you're using stylistic formatting inside of BigQuery, you're adding commas to things, you could be unintentionally having this data be treated like a string in this particular case. And if you haven't worked with adding two strings together, it might not necessarily do what you'd expect it to do. So, say adding rows one and two, 45 billion and 20 billion, that might not give you 65 billion, that might give you 45 plus 20 in quotes or something like that, you know, concatenating strings together. So again, be very careful with
Play video starting at :3:9 and follow transcript3:09
unintentionally converting your your data types through some fun formatting functions and leave the formatting and stylistic elements to your visualization tools, or if you have a final reporting table. They are not doing anything downstream of, you can do it better.
Play video starting at :3:24 and follow transcript3:24
Okay, getting a little bit more advanced. So, here's a really interesting concept. There's two things that are presented here. So what we want to do first, is create a calculated field that represents income. And to do that, much like using a calculator, you can do revenue, minus expenses, and we're throwing that into parentheses. And we're saying, here are the result of those subtractions, and again BigQuery is going to go row by row, and calculate that for you on the fly, and these calculations can be much more complex than you see here, and the result of that is going to be income. Now, here's the really interesting part. Say if we wanted to filter for everything, or income is greater than zero. Well, the interesting part where we're using the filter is BigQuery does not know yet of what this field called income is, when it first looks at the dataset and attempts to filter out those results. So how the actual query engine works is, it doesn't read through your query from top to bottom, it will rip through it in kind of a programmatic way. So, immediately it's going to go, all right what data table am I looking at, and immediately it's going to jump to the IRS 9902015 table. Once it dives into there, it says, okay, well, before looking at all the columns that I need to return, let's see if I can just save some work by not including a ton of these rows in my result. So I'm going to filter this thing as early as possible. Which spoiler alert when we get into the performance section in our later course, is one of the things that you're going to be practicing. So immediately it looks and says, okay, cool, I'm going to find this income field and I'm going to filter it for everything greater than zero and it's going to save me a lot of work.
Play video starting at :5:6 and follow transcript5:06
And then it says, man there's no income field that's here. What's going on? And that's because that field doesn't exist yet. That's something that you've calculated, and it happens kind of after after the filtering has taken place. So, long story short, in most cases, you can actually use that alias later on in the query, as you see there you can ORDER BY income descending. But if you actually wanted to filter on income in the WHERE clause, take a guess on what's one of the things that you could do.
Play video starting at :5:40 and follow transcript5:40
And if you guessed copying and pasting total revenue minus total functional expenses, that entire parentheses field there into the WHERE clause, you would be correct.
Play video starting at :5:50 and follow transcript5:50
In later courses in the Achieving Advanced Insights with BigQuery course in this specialization, a lot of you might be thinking that, on the previous slide of hard coding something like that twice
Play video starting at :6: and follow transcript6:00
violates the "don't repeat yourself" mantra of good development practices. And that's when we'll introduce things like breaking apart queries into separate components and doing things like subqueries. So stick around for that if that's something that really interests you.
Play video starting at :6:16 and follow transcript6:16
Okay, so as you might expect, just doing one column, returning one column, isn't that fun. Let's see if we can't get some more data. So we're going to bring back revenue, we're going to bring back the employer identification numbers, so we know who's responsible for this revenue and we also want to see whether or not they're a school. EIN operateschools170cd is not that appealing to the eye. So we're just going to rename that using an alias called is_ school.
Play video starting at :6:43 and follow transcript6:43
But, there's something wrong with this query after you execute it, and if you're best friends with the query validator, it might help you out. But, take a look and see if you see anything that's missing.
Play video starting at :6:56 and follow transcript6:56
So, if you said comma, you were exactly correct. So we're missing a comma at the end of revenue. That first column that we're pulling out, all these columns are going to become separated, and if you said that we had an extra comma at the end, you're exactly correct. So the number one most common error that everyone, including myself, who's been writing SQL for many, many years, are going to run into is your trailing commas and your missing commas. So make sure that you actually update those. So here you have select one column, comma, a second column, comma, a third column, and then no additional comma after that. And again, we'll be using the Query Validator in a lot of your labs as you're going to see.
Play video starting at :7:43 and follow transcript7:43
And as you see here now, in addition to the revenue showing up, we also have the two additional data fields that we included in our SELECT clause. And here you have one school, It looks like row 10, out of those as well. That's an interesting insight. So from there, you can immediately glean that for 2015, sorting from revenue highest to lowest.
Play video starting at :8:4 and follow transcript8:04
This is number whatever that EIN number is, that school has the highest revenue of all schools for 2015 because there's no EINs above that that, are flagged as schools.
Play video starting at :8:19 and follow transcript8:19
One of the things you want to avoid because it's not necessarily the best practice, is
Play video starting at :8:25 and follow transcript8:25
you could have had a friend that said, "Hey, I know SQL just instead of adding on one column at a time, that's going to take an extremely long time, just return all the data". So there's a wildcard that says, give me every single column, and that's called the star. So, if you did SELECT star or the asterisk, I call it the star, to return to all of your columns. You're going to get all of that data back, but performance wise, your queries are going to be slower because it's going to have to process all that data and return it to your output, and because BigQuery is consumption based, even
Play video starting at :8:59 and follow transcript8:59
if you don't care about the additional data in those columns, you're still going to be consuming that resource or returning more bytes of data than you might actually want to use. So avoid using SELECT star.

# Filters, aggregates, and duplicates
Okay, so back with a little bit more details about the WHERE clause. This is more of a recap of what we went through in great detail before, but why can't we use is_school is equal to yes for the filter here? And if you remember, that's because the alias is not known at that filtering time. So that's where we're actually having to specify
Play video starting at ::21 and follow transcript0:21
the entire field. Operatessschools170 code or cd is equal to yes. And be very careful that your data is in capital Y format and not lower y or Y-E-S, or something like that as well. That's a very, very common trip-up as well. Great. So now we have just schools in those top ten revenue results. Okay, moving on from kind of simple formatting functions as you saw before. Let's talk about some aggregation functions that can do some cool mathematical operations like total revenue across all of 2015.
Play video starting at ::57 and follow transcript0:57
So aggregations can perform calculations over entire sets of values. That is to say, if you had 1.5 million charities in your tables and you just wanted to collapse or sum together or count or get a max in all of those rows, you can do that through these aggregation functions. So for here, we're invoking five of them. We're saying, sum the total revenue, but then average it and count the number of employers and then count the distinct number of employers, much like we did in that exploration lab, and then give us the
Play video starting at :1:36 and follow transcript1:36
maximum amount of employees that one of these 2015 filers has and if we have that information here. So the total revenue for all US nonprofits for 2015 is that number featured there and the average revenue is there. And the number of nonprofits and the number of distinct nonprofits and the one who had the most employees, almost 800,000. Now, if that average had too many points after the decimal, what you could do again is round and one of the interesting point here is you can actually nest functions inside of other functions. So SQL is going to
Play video starting at :2:17 and follow transcript2:17
go ahead and produce the average of that total revenue figure. And then after it's done that, it's going to look outside and say, okay, well, we're not done with our work yet. The user also wants us to round this to two decimal places. So ROUND is another function that'll basically take two inputs here. And you can basically have your first input be total revenue and then the amount of decimal places you want to appear after it. So again, this is kind of merges between stylistic, as we were talking about with the pitfalls for the format, and also potentially necessary if you just wanted to reduce it to those two. But again, possible tradeoff with precision. If you are storing this and doing downstream calculations on that. So again, be careful with round as well.
Play video starting at :3:1 and follow transcript3:01
As we mentioned previously in the lab in that we worked with, you can investigate that uniqueness with COUNT and then DISTINCT on that field as well.
Play video starting at :3:10 and follow transcript3:10
All right. So an absolutely key concept when we talk about aggregations is, when you mix together fields that aren't aggregated, say like the identifying number for the charity, EIN. And then the count of all the charities for 2015, you get a little bit of a mismatch. So what you need to do is aggregate together, or group together anything that's not under a formal aggregation function like COUNT, SUM, MIN, MAX, and then put those field values in what's called a GROUP BY clause. And GROUP BY will immediately and always follow the FROM statements. So here we have, give us the identifying number for the charities and the COUNT of them from the table there and GROUP BY the EIN number. And again here, the point of this entire query is actually to see for each of the identifying numbers, count the occurrences or the number of filings that are present in that 2015 table. So if you're thinking real hard, you could be wondering like,
Play video starting at :4:13 and follow transcript4:13
I would normally just expect to see one filing per each unique organization's number. So this is almost like a data quality check. So you might say like, I don't normally want to group by something like EIN. But for here, I'm going to check the data quality. But before we get into that, again, the key pitfall is don't forget to include that GROUP BY clause. When you start invoking things like sums and mins and maxes, immediately add that GROUP BY, or the validator will let you know when you try to run that query. Interesting, so after we ran that query, here are the results. You see that there are EINs, or organizations, that have seven different records or seven rows that are present in that final year 2015. All right, let's see if we can't get a little bit more insight into why that is. Why on earth would you have seven filings for one particular year for 2015? Okay. Well, let's see how often that does occur. So excluding what we would consider the normal use case, which is having just one filing,
Play video starting at :5:26 and follow transcript5:26
how do we actually write that out in SQL? So if you wanted to exclude an aggregation, after filtering has already been done, so we basically want to say, all right, we've done this aggregation for the counts and we basically want to now filter the aggregation. Filtering an aggregation is done by using a special SQL clause called the HAVING clause. So whereas the WHERE clause filters out rows pre aggregation, as we mentioned before, this is immediately going to the dataset. And the query engine says, all right, cool, what can I lose from here immediately? That's done in that WHERE clause. After you've done the aggregations, after that work is done, that's where you can have that HAVING clause. And why am I making such a big deal of this? It's because now we can use an alias in the HAVING, because some of the work has actually been done and that field is then recognized as the alias as well. So HAVING is very, very, very useful when you're filtering aggregations. So you want to say, all right, well, you showed me that in the previous slide. There are all of these different individual charities with these seven filings. How often is that the case?
Play video starting at :6:35 and follow transcript6:35
And here, after we execute that, and just looking at the paginated results there at the bottom, almost 18,000 instances or 18,000 different
Play video starting at :6:47 and follow transcript6:47
charities. Another way to get that instead of looking at the paginated results, is tracking the actual queries input and output, and that's in the BigQuery results panel. You can actually click over and click on Explanation. And in the third course, Achieving Advanced Insights, you're going to get a very deep dive into what this query plan actually does, because this will help you performance optimize your queries. But if you're looking for just the amount of rows that came in, the amount of rows that came out, they're in red. That's exactly what happens. The processing and shuffling that occurs between each of these different stages is a very, very exciting topic in the architecture lecture in the third course. So a lot of you are still wondering, all right, you still haven't told me why there are seven records. Please tell me why there are seven records.
Play video starting at :7:37 and follow transcript7:37
So we're going to zoom in on one particular EIN here. And if we wanted to zoom in on one, how you actually do that is by filtering every single record that's returned just for that particular condition that you want to meet. And then here, you want to make sure that you're using the correct data type. So if EIN is stored as an integer or if it's stored as a string, strings you need to have in single quotes. Integers, you do not want to put in quotes here. So keep that in mind. Okay, so here's the result. So we have seven paper filings for one EIN for 2015, which is the tax period of 2014 since you file your taxes a year after the actual tax period. And we pull up another interesting field called the tax period that's associated with the filing. So in the 2015 calendar year, we have this EIN 2008 to 2014 filings as well.
Play video starting at :8:40 and follow transcript8:40
So thinking about
Play video starting at :8:43 and follow transcript8:43
why that is, to cut to the chase, the real
Play video starting at :8:49 and follow transcript8:49
kind of answer that's staring everybody in the face is human error or dirty data or an organization is submitting more than one tax period filing. Because maybe they knew they could get caught up to speed or maybe there's some kind of refiling. If there's another flag in here that says, this was a corrected filing or something like that. There could be many, many cases. But the bottom line is unless you can tease that out with other columns in the data, you need to go to the subject matter expert who understands how this data was set up in the first place. But you want to be aware of anomalies like this. So when you start totaling things up like, give me the sum of all revenue, or the average of all revenue for across all of your different employers. If you have something that occurs seven times, that's going to skew your results.
Play video starting at :9:39 and follow transcript9:39
So just keep in mind for situations like this. Now if you wanted to handle that inside of SQL, say, if you just wanted to pull the latest 2015 filing for that tax periods. So you just wanted to pull that record number five there. How could you extract just the tax periods for 2014? And again, that's for calendar year 2015. So if you just wanted 2014, you can invoke a date filtering function. So you basically want to say, all right, well, I want to treat this field as a date since it's not currently stored in very friendly year, year, year, year, month, month, day, day format. We need to invoke or use
Play video starting at :10:23 and follow transcript10:23
a parsing function that basically says, hey, treat this tax period in the middle as a string instead of an integer, which it's currently stored as. As that's we're using that cast function and then you're nesting that inside of a date parsing function, basically saying, all right, now we have this string that I want you to interpret it as. Yea. Yea. Yea. Yea yea. Month. Month. And that's the percentage sign while a percentage sign lowercase m there as well. And then once you can actually pass that as a date, treat that as tax period. And again we cannot use that tax period alias for filtering in the where clause unless you had separated this into two separate queries
Play video starting at :11:8 and follow transcript11:08
or as part of a subquery that you're going to learn a little bit later in later courses, we're going to repeat that exact same
Play video starting at :11:15 and follow transcript11:15
function in your filter excepts here. We're going to basically say, all right, well, once you've returned that result of what that tax period is,
Play video starting at :11:24 and follow transcript11:24
what will the only ones that we want are the ones that are for 2014, for this particular calendar year table of 2015. And how you use that is yet a third function, which is the extract function for dates so you can extract from that date filled up a year. Now why are we bothering to do all this date parsing? Why don't you use something just like a left for and then just grab the first four characters and, and the benefit is kind of
Play video starting at :11:55 and follow transcript11:55
that if we can properly treat this field as a date, then we can do normal operations like extracting the year or doing date difference. It just allows it to treat it as it
Play video starting at :12:9 and follow transcript12:09
as it should be, which is as a, as a date field. So this kind of preprocessing, again, you would want to do when you're ingesting the data or you want to do it before you store a final reporting table as well. So in future modules, I'm going to teach you how to clean up and transform this data, both using SQL, as you see here, but also a little bit
Play video starting at :12:30 and follow transcript12:30
easier if you don't have these functions immediately recalled to memory through a UI, a user interface called Cloud Dataprep. And again for these functions, PARSE, DATE, and EXTRACT, I don't even have a lot of these committed to memory. You can just search online for Google Standard SQL BigQuery, and that'll take you to that one page guide.

# Data types, date functions, and NULLs
Okay, yeah, if you looked at that and said, hey, I was following along perfectly fine until we got to that slide, that's completely fine. Parsing and converting dates is just nuts. So you'll refer to that one-page guide, as I mentioned. And there's a ton of different options for converting between data types and parsing those dates. There's a link down below to a lot of those functions as well, but let's take a look at a few. Before we do that, just understand the fact that there are different types of data, data types, that are stored within your tables. You could have numeric data, like your integers, so somebody's salary could be stored as numeric. Or you just have strings, like their first and last name is string data. As you saw, the nuances there, you need to have those stored. And you need to operate, if you're trying to do equivalency operators like name equals something, they need to be in single quotes. And then dates, again, are in that year, year, year, month, month, day, day format for BigQuery. Other values, of course, you can have your Booleans, your yes/no fields. And we'll introduce these last two in great detail in our architecture lecture as part of course, three, which is your array, which is kind of a series of values, and your structs, which are flexible containers. Don't worry about those two yet.
Play video starting at :1:22 and follow transcript1:22
Okay, so say you were given a dataset that wasn't particularly clean and you had to convert or treat data types as another before storing it into an alternate reporting table. This is super useful for kind of transforming your data. How would you use that? So if you're familiar with a little bit of SQL, you also probably have ran into the CAST or convert function. So in BigQuery and Standard SQL, It's the CAST function. So you have "12345", which is a string of numbers. You want that to be treated as an integer, boom, The result is that an integer, 12345. Or if you have something that's stored as 2017-08-01, treat that as an actual date and allow me to use these special functions that expect data in a certain type. Or if you want to convert it to string. Or the bottom example is an interesting one. If you're unsure of the data that's coming in, so say you don't know that it's apple. You ask somebody to put in their I don't know, their age or something like that. You're expecting it as an integer and you're not doing any kind of, you know, prefiltering or validation. You can use something that like a SAFE_CAST, so instead of erroring out, you can basically say, well, I'm going to convert it, and if it doesn't match what I think it is, I'm going to actually have it be an empty value, or a NULL.
Play video starting at :2:43 and follow transcript2:43
Now, NULLs are super interesting, right? So they are valid values, but a NULL is the absence of a value. It's not "", it's not an empty string, and it's not,
Play video starting at :2:57 and follow transcript2:57
you know, like if you hit the spacebar once, it's not like just a single space. It is the absence of data. And then you'll see a lot more of that when we get into a lecture on JOINS where you're actually merging two different datasets together. And when they don't have matches, then you actually have valid missing values or NULL values. So if you work a lot with data, you become very, very familiar with NULL values. And actually how to deal with them
Play video starting at :3:26 and follow transcript3:26
when you're trying to match and just find those NULLs.
Play video starting at :3:30 and follow transcript3:30
So let's get into a little bit of that now. If you want to match on those NULLs, like for example, you want to find all of the values where the state
Play video starting at :3:39 and follow transcript3:39
is missing for these charities because you're saying, hey, well, you know, if it's a US charity, it should have a state value. Now, you would normally think that if you're going to use an equivalency operator, like state equals California or state equals New York, that you would use an equal sign. So NULLs, again, super tricky beasts. They cannot be equivalent to anything, not even themselves. So you actually have a separate set of operators for NULLs and that's the IS, so state IS NULL or state IS NOT NULL. So just be familiar when you're using
Play video starting at :4:17 and follow transcript4:17
NULLs, you'll be using the IS. And there's special functions that handle NULLs as well, so if you wanted to parse in data and if it was NULL, then set it to zero or something like that. There's those NULL functions as well that you can wrap around your fields.
Play video starting at :4:34 and follow transcript4:34
So for the state example, take a look at these. These are U.S. charities, or at least the ones they're they're filing with the U.S. And they're missing the state value here. So we have some cities,
Play video starting at :4:54 and follow transcript4:54
Canada for some of these cities as well. But they're missing those state values, which is really interesting.
Play video starting at :5:2 and follow transcript5:02
So not all countries could necessarily be using a state value. For example, maybe Canada doesn't use a state value, or Italy doesn't use a state value much like the U.S. would, but they're operating or at least doing business in the U.S..
Play video starting at :5:18 and follow transcript5:18
As we mentioned briefly before, year, year, year, year or YYYY-MM-DD
Play video starting at :5:24 and follow transcript5:24
is the expected format for dates. And that unlocks a ton of different date functions that you can then use on that date itself. So extracting the year, extracting the week, extracting the day, and that's the second row there. The EXTRACT function is very powerful if you want to add or subtract or take the difference between two intervals of time. Truncate, so you only want to look at that month or you only want to look at the year. Or format into a different format, you can do that as well. So once you get it into that proper DATE or DATETIME data type, you unlock a lot of really, really, really powerful things that you can do. So dates, it's a necessary evil inside of SQL, but you'll get familiar with them.
Play video starting at :6:10 and follow transcript6:10
You can parse these string values. So for example, before we were just casting between values for data types. Now we can actually do some fun things like string manipulation functions. So you're probably already familiar with concatenation. So if you had two string values, much like "12345" and "678",
Play video starting at :6:30 and follow transcript6:30
you could merge those two together. Or say you have two fields, one's a first name, another person's a last name. You could string together their first name and last name to get first and last name.
Play video starting at :6:41 and follow transcript6:41
Or if it's just, a lot of these are kind of self-explanatory. If it ends with the letter e, then return true or false. A very popular one is convert everything to lowercase or convert everything to uppercase. So in the example we had way earlier on whether or not it IS school or IS NOT school for our charities. You can think of, well, I don't know whether or not in my dataset it's going to be capital y or lowercase y. I'm just going to put everything to lowercase and then match on a lowercase y. So you would do LOWER parentheses - whatever the field name was for that school I think it was like OperatesAsSchool - and then you could set that equal to lowercase y. And that will allow you to not worry about guessing. And then of course, you could use regular expressions in here as well.

# Wildcard filters with LIKE
Okay, speaking a little bit more on like, wildcard matches. So if you didn't want to use a direct equivalency operator like a charity name is equal to a Kaiser, what you could do here is you could use a wildcard operator for this case, matching on a string where it contains the word to the letters help. H E L P, somewhere in the charity name. As I mentioned before, if we didn't want to worry about whether or not it was capital H or lowercase H, we're going to wrap the entire condition in the lower function here. So, all the names that are being passed into that lower function are being converted to a lower case, and we're matching them against that hardcoded string value of help. And the percentage signs there mean, any number of characters can
Play video starting at ::47 and follow transcript0:47
come before the H, and any number of characters can come after the P in help. If you wanted to just say one character comes before, or one character comes after it, you would use the underscore wildcard placeholder. That's a single placeholder. Okay, so let's look at the results. So the results here's a lot of charities with help in the name. Now you might look at the top three and go, oh, come on, that doesn't necessarily, that's not help, right? And well, according to your conditions it was. Any number of characters could come before, and any number of characters could come after, including help itself nested inside of another word. This time might want to get more specific. Now we would say, alright, well it has to begin with help. So you remove that wildcard operator in front of the H, and now you get everything that just begins with H E L P.
Play video starting at :1:43 and follow transcript1:43
So a lot of powerful things that you can do with filtering and a lot of these SQL functions and operators. So you build out your arsenal, the more practice that you get, like the practice that you're gonna get in this lab, or the practice that you use on other public BigQuery datasets, is just going to pay you dividends in the long run. Getting familiar with what are some of the different things that you can do with the data. And the good news about doing this inside of BigQuery is it will execute very fast and you can move on to your next query. So these functions, statistical functions, analytical functions, like window functions and partitions, and user defined functions like doing things in JavaScript, we're going to save these three topics for the later courses, as we focus on building up more foundational concepts like processing and transforming and cleaning up the data. Mainly because I really want to show you the cool tool that is Cloud DataPrep as well, for doing a lot of these same
Play video starting at :2:35 and follow transcript2:35
processing steps for data, but doing it through a cool UI. So if you're interested in a lot of these statistical approximation functions and data partitioning, stick around for the second and third courses in the specialization.
Play video starting at :2:50 and follow transcript2:50
Now looking back, this is one of the first modules we really got to dive into some of the core capabilities of BigQuery. As you've seen, exploring massive datasets with SQL, is one of the things that BigQuery does best, and that's what you're going have to master as a data analyst. And be sure to use standard SQL mode, and make best friends with that query validator. Lastly, if you're looking for other new datasets to explore with even more pre-built example SQL queries, search for BigQuery public datasets to see the full list. Now let's test your knowledge with an interactive lab. We've written some pretty ugly and broken queries for you to fix on our IRS dataset.