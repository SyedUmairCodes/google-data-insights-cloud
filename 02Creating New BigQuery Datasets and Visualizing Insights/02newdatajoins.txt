# Ingesting New Data into BigQuery

Now, so far we've only queried datasets that already exist within BigQuery. The next logical step after you're finished with all of these courses is to load your own datasets into BigQuery and analyze them. That's why in this module, we'll cover how you can load external data into BigQuery and create your very own datasets. First, let's cover the difference between loading data into BigQuery versus querying it directly from an external data source. As you can see on the left, there's a lot of different file format types and even systems that you can actually ingest and grab data from, and then load permanently into BigQuery-managed storage. Just to name of few very common staging areas, Google Cloud Storage, we could have your massive CSV files stored into Cloud Storage buckets, which is very common, or Cloud Dataflow jobs. Your data engineering team has set up these beautiful pipelines, and it's part of one of the steps in the pipelines. You can have that data right out or materialize itself into a BigQuery table for analysis. That's very common. and as you saw as one of the UI layers for Cloud Dataflow, that Cloud Dataprep tool, that you got a lot of practice with in the last course, does exactly that. It will invoke that materialization step for Cloud Dataflow and then write that out to BigQuery-managed storage. Other Google Cloud platform tools, big data tools like Cloud Bigtable, you can export or copy that data from Bigtable into BigQuery-managed storage. Of course, you can manually upload through your desktop or a file browser and ingest those tables into BigQuery-managed storage. Why do we keep mentioning the word managed. That big concept, that big icon that you see there in the middle, is a key core component of the BigQuery service. As you mentioned in one of the earlier courses, BigQuery is two components. It's the query engine that will process your queries, and it's also the data management piece behind the scenes, that handles, and stores, and optimizes all of your data. Things like caching your data, storing it into column format, and compressing those columns, which we're going to talk a little bit more about in the advanced course on the architecture of BigQuery, and expanding the data, making sure that it's replicated, and all of these things that are traditional, a database administrator would handle for you. The BigQuery team here at Google manage that for you behind the scenes. Why am I making such a big deal about managed storage? Because you might guess, hey, all right, cool, it's managed storage, I don't have worry about that. When is my data never going to be in managed storage? The answer is, it could quite possibly never even hit managed storage if you connect directly to the external data source. This is the mind-blowing concept. You can write a SQL query and that SQL query can be passed through, and underlying your actual data source, could be a Google Drive spreadsheet that someone is maintaining, and that data is not ingested and permanently stored inside of BigQuery. That's an extreme case, because naturally you can see the caveats of relying on a collaborative spreadsheet as your system of record for a lot of your data. But this is a common occurrence for things like one time extract transform, load jobs, where you have a CSV that's stored in Cloud Storage. Instead of ingesting that data and storing that raw data inside of BigQuery, storing it in two places, Cloud Storage and BigQuery, you instead query it, performed some pre-processing steps, cleaned it all up, and then at the end of that query, store the results of the query as a permanent table inside of BigQuery. That's one of the common use cases that I can think for creating or establishing this pointer or this external connection. Now, as you see that big arrow over BigQuery-managed storage, you're just and the query engine, you get none of the performance advantages from BigQuery, the managed storage piece, and a lot of other drawbacks. Let's cover some of those limitations. It's performance disadvantages, there's a lot that goes into the special sauce of the BigQuery architecture behind the scenes. What makes it much more performant to store your CSV data, ingest it permanently to BigQuery, as opposed to keeping it out on say, a Google spreadsheet or Google Cloud storage. A lot of those compression algorithms and the architecture of BigQuery, how it stores data in column format, we'll cover a lot in the architecture lecture coming up in the next course on advanced insights. But one of the key things that should hopefully scare a lot of you away from using Google spreadsheets as your source of truth for your underlying datastore, could be data consistency. If you're writing out of BigQuery query, as we mentioned in the previous slide, you have a BigQuery query that then is reaching out to a Google Drive spreadsheet. If you have folks that are editing that spreadsheet, the query doesn't necessarily know, hey, this is when I accessed it at this particular timestamp, and this is what the data was. If you have data influx or inflate, since it's not managed natively by BigQuery itself, there are few checks in place on whether or not the data that you're pulling was the data you've expected when it was last updated in that particular spreadsheet as well. A lot of features that you can actually enable inside of BigQuery like these table wildcards, when we uncover unions and joins inside of our merging datasets lecture, are unavailable outside of storing your data directly inside of BigQuery.
Play video starting at :5:47 and follow transcript5:47
We've largely discussed batch loading a CSV or massive CSVs into BigQuery, but know that there is a streaming option available through the API, where you can actually set it up, where you can ingest individual records at a time into BigQuery-managed storage, and then run queries on those as well. The streaming API is well documented, and you guys can access that if you have a streaming or near real-time data need for your application.

# Introducing JOINs and UNIONs
Now one of the most popular topics in SQL has had to mash up multiple data sources together in a single query to answer more complex insights. Now in this module we'll tackle how to append additional historical datasets together, through UNIONs, as well as how to join together different datasets horizontally through SQL JOINs. Let's walk through the basics and I'll highlight some key pitfalls along the way.
Play video starting at ::23 and follow transcript0:23
All right, so now let's move into one of my most favorite topics to talk about which is enriching your dataset through the use of JOINs and UNIONs. And if your SQL guru, there's a couple of different tricks and tips that you can use inside a big query that I think you'll like. But let's go over some of the basics. A JOIN, for example if you have the tables for the IRS annual tax history for 2015, but you want to get the name of the charity. Now that's in two different tables. You actually have to link those two tables together, in SQL we call that a JOIN, and you do that on a common identifying field that shared between the two tables. And there's a lot of caveats when it comes to whether or not those fields are duplicative or which type of JOIN you're going to use. And we're going to get into all that nuance as part of this module. Now a little bit simpler than that is a UNION which on the right. If you have historical data that shares the same schema, you can actually append records or mash them together vertically, I like to say, through what's called the SQL UNION. So combining both of these two concepts together, you could actually bring together the annual tax filing history for 2012 or however long the dataset goes for 2012 to 2013, 2014 to 2015, and so on. And get an enriching information about the organizational details that's not present in the individual filings by using a JOIN. So let's explore these concepts a little bit more in depth with a walk through example.
Play video starting at :1:52 and follow transcript1:52
So the example that we're going to use to illustrate JOINs and merging of data is going to be temperature and weather station data. So the example that we're going to be using for this walkthrough is the NOAA weather dataset. NOAA is research agency in the US which tracks the patterns of weather and does a bunch of other cool things with meteorological studies. And as such, they have a lot of really, really good weather data. So we have temperature recordings, not just for today, but all the way back into the 1920s for a lot of their different weather stations all around the world. So you have these different temperature readings, and you have the locations of the weather stations that they belong to. And as you see with the dividing line, those two pieces of data are in two separate tables that we're going to have to join together.
Play video starting at :2:37 and follow transcript2:37
So it's many more tables than just two. So for those daily temperature readings, it actually goes back to 1929. So there's an individual table for the annual readings from 1929, [LAUGH] and I believe this is hourly, every hour, right? So you're talking millions and millions of records, all the way to the current day. And then on the right hand side you have a look up table that has the information about the stations that took those readings. So this is like Wake Island Airfield. The latitude and longitude and a bunch of other look up information that we don't want to repeatedly store in those daily temperature reading tables that we're going to use something like a JOIN to enrich those two together. So let's talk a little bit more about the nuances of that.
Play video starting at :3:19 and follow transcript3:19
Okay, so we need to find a unique identifier for our weather stations before we can even hope to join them against another table. So when you inherit a dataset, this is honestly one of the trickiest things to uncover, is how do you uniquely identify a row in a dataset that you've been given? So there's two fields that look like they're identifiers, the usaf for the United States Air Force, weather identifier there, usaf. And then you have the wban, the wban in there. So what you likely saw in the first lab as part of the first course is introduce you to counting, duplicative, or distinct records. And we use two simple account functions as you see there in the SQL code. The regular count, and the count distinct. That's a quick way to see whether or not those records are the same, meaning that it is unique for those row counts. And as you see here with the red X is they are not. So you have an interesting dilemma. If you don't have any one field that can uniquely identify a row, what could you potentially do?
Play video starting at :4:23 and follow transcript4:23
So two general options. One, you could create your own primary key. You could insert arbitrary row number and have that be unique identifier for that record set. But if we're going to have any hope of joining it against a different dataset, maybe we'll have to make a deal with what we have. And one of the things that you can do is create a combination key. Where the combination of these two identifiers, the usaf field and the wban field together kind of smashing them together through that concatenation or that merging, is in itself a unique identifier. So you can see for that first record, right? So for the 912450, you can see row 1 and row 4 are duplicative for the usaf. But if you merge them together with the wban, even though the wban has 41606 repeated three times, the combination of both of those fields together uniquely identifies them. Okay, so how are we actually going to join that combination key on all of these different tables? We need to somehow UNION potentially bring all these data tables historically together, mash them vertically. But that might be a really long effort. So let's cover exactly what a UNION is first before we talk about the syntax to how to do it. So UNION in brief, if you have tables with the same schema or at least from the field that you're selecting across the tables are the same, what you can then do is apply a UNION and it mashes those two tables together vertically. So if you have records from the year 1929 and then 1930 into two tables as you can see the result they're added vertically, it's going to give you a consolidated table with records from both 1929 and 1930.
Play video starting at :6:11 and follow transcript6:11
So let's go into a little bit more of nuance. So there's two different types of UNIONs. You can have a DISTINCT UNION, or you can have what's called a UNION ALL. So if you just wanted to blindly mash together all the records, without worrying about whether or not one table had a duplicative record from another table, you would be using the UNION ALL. So now say if you wanted to actually be mindful of duplicates across both the different tables, you would be using a UNION DISTINCT. Now here, we shouldn't see repetitive temperatures across different tables, since those temperatures are fenced in by the year of the particular table. But if you just want to be extra cautious, you could do that UNION DISTINCT.

# Use table wildcards for easy merges
Okay, so here's the dilemma. You have quite a few tables that you need to bring together. This is the syntax for the union, select all the fields we want from one of these particular tables. Union distinct, just write it out and then you just do another table and union distinct, do another table and union distinct, do another table. [LAUGH] And if you have more than 10 tables, your fingers are going to get tired of typing all these tables over time. So I don't really want to type a 100 unions and it's going to make my code just extremely long vertically to read. So it must be a better way. And now we're going to introduce the concept of a table wild card. This is pretty cool. So much like using that like operator when we looked at finding the charities that had helped in the name. You can actually operate over the table names and you're from statement and do a wildcard matching the union to you see this asterisk here, this is pretty cool. So let's cover the SQL syntax up there on the right. So the same thing, you're selecting the fields that you want from all the tables. And then you're adding the from clause the datasets there. And for the table, you see it begins with the prefix gsod and then there's an asterisk. So any tables that are in that dataset and its entirety that match that prefix are going to be included. And because the dataset is very original structured, so it's going to be year, year, year, year, for each of those different tables. It's going to bring together all of the historical weather data tables from 1929 to current. You're talking about a lot of millions and millions of records just with one single line of code there.
Play video starting at :1:38 and follow transcript1:38
All right, so what happens if you wanted to filter out for just a subset of the tables? So, say, you just wanted to find and match together all the temperatures from 1950 or after 1950. Now here's another reserved keyword that's specific to big query. You can use the table suffix to grab what you're actually matching on in that wild card. And then you can use that table suffix in normal SQL operations. So here you can say, all right, well, match everything but only include those tables that are after 1950 here.
Play video starting at :2:14 and follow transcript2:14
And of course, you normally want to be as granular or make that prefix as long as you can for performance reasons. Just you're limiting the amount that you're actually matching on. So table suffix in the wild card are two very useful concepts and that's the subject of our next key message. So if you have many, many, many, many unions across datasets that have labeled data tables with structured standardized names, make use of that table wildcard. And also what's really, really nice is this table suffix, you can use that to filter out the tables that are actually included. And in addition to filtering on that in your where clause you can actually return the table name in your select statement by just calling that stable suffix. Okay, so here are some pitfalls when it comes to doing unions inside of SQL. So keep in mind that duplicative records piece that we mentioned before. When you want to remove the duplicative records, when you're mashing multiple tables together, you'll be using a union distinct. And when you want to just have all the records, no matter if there are duplicate records across multiple tables, you'll be using that union all. Now it is required that you choose a distinct or an all option. So, as you might imagine, you're hoping that the way these temperature recordings were recorded over time that the field names haven't changed. Or the amount of fields that were required hasn't changed over time. Because if they do, when you're matching these tables together vertically. If you're trying to put tables that don't have the same amount of columns, you're going to get a mismatch and you're getting an error in your union. So it's very good to do a pre processing exercise to make sure that unlike what we saw in cloud data prep, where you can have fields that are present in one day to step down and the other. In a true SQL union, those tables need to match up exactly on the count of columns. And now you can do, technically, you can have different column names and match together based on what's called the index of where that column appears. But generally what I like to do just for my own sanity is make sure the names and the count of columns matched as well.
Play video starting at :4:18 and follow transcript4:18
Okay, so recap, we have joined, the union, rather, we've matched together all that historical data from the past all the way to the current. And that's a lot of temperature readings. But we still don't know where those temperature readings came from. And that's where we're actually going to be joining this massive amounts of consolidated temperature recordings with another data set that is just a single table on that station reading information.

# Linking data across multiple tables
Let's work on joins. How do we link across multiple tables? Conceptually, a join combines data from separate tables into one table. Now technically a union will do that too, because it will consolidate that output. But joins, you can think about doing it horizontally. You can actually add in more fields, things like the charity name, for example, in your IRS example, or the station name in this weather recording example, and this is your join syntax. You're selecting the fields that you want from both tables. You'll notice that a., b., we're going to get into that in just a second, from, and you see the union wildcard here and all of that historical gsod* year tables. You're specifying that as an alias, you're saying AS a. The reason why you're going to do that has become apparent in just a second. You're joining that on a key which we're going to cover into the stations data. We're saying that As b. Why are we using these aliases a and b? Is because if there are ambiguous field names, like say the word name or say the word station is common across both tables. When you join these two together and you're adding new fields, you can't have any name collisions. So having an alias actually specifies where that comes from. It's actually technically not required for a. field name if that name is unique across all the tables that you're joining. That as the actual joint itself, now the condition is match these records, bringing these tables together horizontally, on the weather recordings, which is a, a. station, is equal to the b.usaf. That's again, you can actually match any fields that have names that don't match. But again, it's just going to look at those values, and that's the case here, we see the station name actually doesn't match the usaf name in the stations table. Also you can have that additional piece of the join key, a.wban equals b.wban. There's a lot going on there. Then of course we apply a filter all the way at the bottom that just does a bunch of different things. It filters for only US temperature readings and table data that's after 2015 and the US State is not null. But the main thing that we want to focus on right now is that join condition. There's a lot of things that are going on. One of the things that we haven't covered yet is what type of join and then we're actually doing to link these two things together.
Play video starting at :2:35 and follow transcript2:35
Let's do a quick review before we get into the different types of joins. The top table listed in the red box there, those are all the fields that we're pulling from the individual temperature recordings across all those different tables. Then the blue box, those are the fields that are coming from that separate table. Then notice we're listing them all together in that select statement. A general good best practice for you is, if you have table data that's coming from all different sources, you can actually break up in your select statement with a comma. Basically saying, "This is your temperature readings and then these are your station readings as well." That's purely for the readability of your code. Then capital J-O-I-N is your join, we're going into the types in a minute. Then you're joining condition again, you can have more than one join key.

# SQL Join examples

Let's talk a little bit more about join specifics and get into some examples.
Play video starting at ::4 and follow transcript0:04
So there's many different types of joins that are out there, and those of you who have practiced ECO before, the Venn diagram slide is coming, I assure you. So you have an inner join, left join, a right join, a full outer join. Now the best way to think of this is through an example, and I have a handy dandy whiteboard that I'm just going to pull up. Okay, and that's exactly what we talked about here. So inner join is that intersection piece of that Venn diagram. Left join is all the rows from that left side and where they match with the right side. Right join, again, it's never seen used because you're just switching the order in the sequel, literally of your table name. And an outer join is just a full outer join that is just a way of returning all rows from all tables. So, there's actually two major types of different joins, inner join versus outer join. A left is considered an outer join, so it's actually left outer join and right outer join and a full outer join. And again, if you just typed in join into SQL that will default to an inner join, so only when you have matching records. As a review, that's exactly what we just covered with our Venn diagram example.

# Avoiding Pitfalls when Merging Datasets
So now one of the key pitfalls that you could run into, and this is why we talk a lot about eliminating those duplicative records, is doing what we call a many-to-many JOIN. Now in many-to-many JOIN, for example, if we had not gone through the exercise for our stations and weather temperature readings exercise, what we could have done is joined on, not necessarily, a distinct or unique key. So you have five records on the right hand side and you have five records on the left hand side and they all match each other. So how many output rows would actually be returned? Would it be zero? Would it be five? Or could it be potentially way more than five? And the answer is way more than five. So you'd actually get what's called the Cartesian product of five times five. So you get 25 output rows even though your input tables only had five rows. And if you imagine this at scale, if you don't get that joining key correct, that's when you're really just blow out all the resources that are using. Because having a million record table joined even on a 10,000 record table, you could just be exploding by many, many, many factors of ten and get a data set that's out putting way, way, way more rows than what you're expecting. So the end result is a situation that you want to avoid, and the way you can avoid it is the third bullet point here, is knowing the relationship between the tables and your data before you do that JOIN. So let's walk through an example. Here, this is an irs example where instead of joining on the unique identifying, which is the ein, we joined on that tax period. And there's many common tax period values that can be shared across many of those eins. So let's walk through what it actually looks like conceptually. So here we have our two different tables. On the left hand side, we have the 2015 filings and on the right hand side is the organization or charity details table, which just has a tax period and when they last filed. So instead of joining in on ein, you would normally expect your eins to match up one for one, one ein organizational details per one filing. But instead if you incorrectly put the tax period as your joining key, what you've done here is basically said, all right, well, this tax period in this organizational details table on the right matches five records there on the left. So that's great. What we're going to be returning here is just this ein number, the 345352, as an additional column for all of those five. So it's not that bad. And we just have one record here. This query doesn't make too much sense joining on tax period. But what happens if we join on another ein that also has the same tax period of December 2014. Now you see we're starting to multiply, in a very bad way, the amount of output rows that we have. So this one also matches those five records and what this looks like in your resulting dataset is, again, the cross product or what's called an unintentional cross join of your data. And again this is a very small example where you just have to rows cross joining against five and you get the result there. But you can imagine if you have this on a million row data sent, you could potentially just have even a billion or a trillion rows accidentally output in. And this is generally where big query will run for more than 60 seconds or a couple of minutes and then you realize, wait a minute. There's something that's going terribly wrong with our queries. And again, the way to cure this is really understand what are the unique identifying fields in your data set and what's the relationship between all of your different data tables? So you could have 1 to 1. For example, you could have one charity that corresponds to one tax year of filing for 2015 or you could have many-to-one that's what that end represents. Many-to-one could potentially represent if you had multiple years of tax filings, you could have many tax filings per one organization. Or we could have the opposite one-to-many or you could have that many-to-many scenario, where you might need to create through concatenation, or another method, a unique identify or key that you have. And this is why the very first thing that we taught in the first course here is practicing count and count distinct to see which fields in your data set are the actual unique identifiers for those rows. Understanding when and how to use Joins and Unions in SQL is a concept that's easy to pick up but honestly it takes a while to truly master. The best advice I can give you when starting out is to really understand how your data tables are supposed to be related to each other, like customers to order, supplier to inventory, but being able to back that up by verifying those relationships through SQL. Remember all data is dirty data and it's your job to investigate it and interrogate it before it potentially pollutes your larger data set with Joins and Unions. And once you understand the relationships between your tables, use Unions to append records into a consolidated table and Joins to enrich your data with other data sources. Let's practice these concepts and pitfalls in our next lab

# Recap of Google Analytics ecommerce dataset
If you've taken the first course in this specialization on exploring your data with BigQuery, you're already familiar with the e-commerce course dataset that we've been using so far. If you've skipped the first course, here's a brief recap of the dataset they'll be working with. The dataset that you'll use for your hands-on labs is a year of Google Analytics records behind the Google merchandise store, which sells Google-branded merchandise, like sunglasses, and even t-shirts. It has over a million site hits and transaction records to include insights like, what users were close to transacting but never completed, what top keywords are high-value customers using to reach the site, what top channels are customers going through to reach your site?

# Lab Intro: Unioning and Joining Datasets
In this next lab, we're going to focus entirely on bringing data together from multiple different data tables through unions and joins. Specifically, what we'll tackle is merging together historical annual IRS filings and join them against organizational lookup details so you can get things like the business's name. And the end result is it can be a consolidated reporting table that we can query off of. 