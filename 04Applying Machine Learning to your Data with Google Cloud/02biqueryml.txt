# What makes a dataset good for ML?

When using structured data, the basic three step process that we're going to follow is, create a dataset, build the model and then operationalize that model. As we said before, creating and cleaning the data set is often the largest part of this three step process. Then the cooking analogy I like to use here is choose high quality ingredients, prepare them all together with your expertise and serve up a delicious meal. So let's walk through each of these steps in detail. First up, how can we explore and create that high quality dataset? What data ingredients are we working with? Now repeat after me, don't assume that datasets all have high quality or complete data. Keeping with our kitchen analogy, don't think that all the ingredients in your fridge are fresh. Or even if they are fresh, you may not need all of them for tonight's dinner recipe. Now, let's talk about data completeness. I like to draw the comparison of the data set that you just received from your colleagues, a picture you came across in a random photo book. That picture is just like a dataset, actually it is a dataset. It's pixel color values and there's just numbers. How do you think the computer sees an image of a cat? It's just like RGB or red, blue, green values inside of a matrix. Anyway, back to the picture. If I give you this and asked you to see in this collection of pixel data. What do you see? Well, you might say I've got a river and there's a boat, maybe some trees. if you have really good vision, there's a flag in there somewhere hidden on the right. How about now? Now it's super easy, right? That's the Eiffel Tower in France. The window of the image you saw first is highlighted in red. I literally didn't give you the complete picture. Now, it's a trivial exercise with pictures to see when the pieces of the puzzle are missing or they don't fit right. Why would it be so different or harder with data? Now, the point here is that you should always remain critical of your data sources. Where did they come from? Why were these fields chosen and not those? Am I missing any data? You don't want to draw inference or build a model that draws inference before understanding the complete picture of your dataset. Now that you understood the origin story of your data set as a whole, let's go through a set of five rules that we can test out each of our feature columns against.

# Choosing good features
Here's our list. Good feature columns must be related to the objective, known at prediction-time, numeric with a meaningful magnitude, have enough examples present, and lastly, you're going to bring your own human insight to the problem. First up, a good feature needs to be related to what you're actually predicting. You need to have some reasonable hypothesis for why a particular feature value might matter for this particular problem. You can't just throw arbitrary data in there and hope that there's some relationship somewhere for your model to figure out. Because the larger your dataset is, the more likely it is that there's lots of these spurious or strange correlations that your model's going to learn. Let's take a look at this. What are the good features shown here for horses? Well, it's a trick question. If you said it depends on what you're predicting, you're exactly right. I didn't tell you the objective of what we're after. If the objective is to find what features make for a good racehorse, you might go with the data points on breed and age. Does the color of the horse's eyes really matter that much for racing? However, if the objective was to determine if certain horses are more predisposed to eye disease, eye color may indeed be a valid feature. The point here is that you can't look at all your feature columns in isolation and say whether or not one is a good feature. It's all dependent upon what you're trying to model or what your objective ultimately is. Number 2, you need to know the value at the time that you're doing the prediction. Remember the whole reason to build the ML model is that you can predict with it. If you can't predict with it, there's no point in building and training an ML model. A common mistake that you are going to see a lot out there is to just look at the data warehouse that you have and take all of that data, all the related fields, and then throw them all into a model. If you take all these fields and just throw them into an ML model, what's going to happen when you're going to go and predict with it? Well, when you go in at prediction time, you may discover that your warehouse had all kinds of good historical sales data, even if it's come perfectly clean, that's fine. Use that as the input for your model. Say you wanted to get, how many things were sold on the previous day, that's now an input to your model. But here's the tricky part. It turns out that daily sales data actually comes into your system a month later. It takes some time for the information to come out to your store, then there's a delay in collecting and processing this data. But your data warehouse has this information because somebody went through all the trouble of taking all the data and joining all the tables and putting it all in there. But at prediction time or at real-time, you don't have that data. Now the third key aspect of a good feature, is that all your features have to be numeric and they have to have a meaningful magnitude. "Why is that?" you ask. Well, ML models are simply adding, multiplying, and weighing machines. When it's actually training your model, it's just doing arithmetic operations, computing trigonometric functions and algebraic functions behind the scenes, and your input variables. Your inputs need to be numbers and trickily your magnitudes need to have a useful meaning, say like the number two that's present there, is in fact twice a number one. Let's do a quick example. Here we're trying to predict the number of promo coupons that are going to be used and we're to look at the different features of that promotional coupon. First up, is a discount percentage, like 10 percent off, 20 percent off, etc. Is that numeric? Sure. Yeah. Absolutely as a number. Is the magnitude meaningful? Yeah, in this case, absolutely. A 20 percent off coupon is worth twice as much as a 10 percent off coupon. It's not a problem. This is a perfect example of a numeric input. What about the size of the coupon? Say I define it as four square centimeters, 24 square centimeters, and 48 square centimeters. Is that numeric? Yeah, sure. So is 24 square centimeters six times as much or more visible than one that's four square centimeters? Yeah, it makes sense. You can imagine this is numeric, but it's also unclear whether or not that magnitude is really meaningful. Now if this was an ad you're placing, the size of the banner ad, larger ads are better and you could argue that that makes sense. But if it's a physical coupon and it's something that goes out and like the newspaper, and then you got to wonder whether or not a bigger or 48 square centimeter coupon really is twice as good as a 24 square centimeter coupon. Now, let's change the problem a little bit. Suppose we define a size of the coupon as small, medium, and large. At that point are small, medium, large numeric? No, not at all. Now, I'm not saying you can't have categorical variables as input to your models. You can, but you just can't use strictly small, medium, and large directly. We got to do something smart with them and we'll take a look at how to do that shortly. Let's go with the font of an advertisement. Arial 18, Times New Roman 24. Is this numeric just because it has numbers in it? No. Well, how do you convert something like Times New Roman to numeric? Well, you could say Arial is number one, Times New Roman is number two, Roboto is number three and Comic Sans is number four. But that's a number code. They don't have meaningful magnitudes. If we said Arial is one and Times New Roman is two, Times New Roman is not twice as good as Arial. The meaningful magnitude part is really important. How about the color of the coupon, red, black, blue. Again, these aren't numeric values and they don't have meaningful magnitudes. We can come up with RGB values to make the color values numbers. But again, they're not going to be meaningful numerically. If I subtract two colors and the difference between them is three, does that mean if I subtract two other colors and the difference between them is also three, are these the same? Are they commensurate? No, and that's the problem with magnitude. How about item category? One for dairy, two for Deli, three for canned goods. As you've seen before, these are categorical, they're not numeric. Now, I'm not saying that you can't use non-numerical values as we said before, we've seen it do something to them, and we look at things that we need to do to them. Using an example, suppose you have words in a natural language processing system, the things that you need to do to words to make them numeric is that you could simply run something that's called Word2Vec. It's a very standard technique and you basically take all of your words, apply this technique to make those words numerical vectors, which as you know, have a magnitude. Each of these words becomes a vector. And at the end of Word2Vec, when you look at these vectors, the vectors are such that if you take a vector from man and you take a vector from woman and you subtract them, the difference that you're going to get is going to be very similar to the difference if you take the vector for king and you take the vector for queen, and if you subtract them, that's what Word2Vec does. Changing an input variable that's not numeric to be numeric, it's not a simple matter, it's a little bit of work. Well, you can just go ahead and throw some random encoding in there. But your ML model is not going to be as good as if you started with a vector encoding that's nice and understands the context of things like male and female, man and woman, king and queen. That's what we're talking about when you say numeric features and meaningful magnitudes. They've got to be useful so you can do the arithmetic operations on them during your ML processing phase. Point Number 4, you need to have enough examples of that feature value in your dataset. A good starting point for experimentation is that you need to have at least five examples of any value before I'll use it in my model. At least five examples of a value before you use it in training or validation or so on. If we went back to our promo code example if you want to run an ML model in our promotion codes that gave you 10 percent. You may well have a lot of examples of 10 percent promo coupons in your training dataset. But what if you gave a few users a one-time discount code of 87 percent off? Do you think that you'll have enough instances in your dataset of an 87 percent discount code for your model to learn from? Likely not. You want to avoid having values of which you don't have enough examples to learn from. Notice I'm not saying that you have at least five categories, like 10 percent off, 20 percent, 30 percent off. I'm not saying that you need to have at least five samples, like four records or five records in a column. I'm saying that for every value of a particular column, you need to have at least five examples. In this case, five instances, at least for 87 percent off discount coupon code that hadn't been used before we even consider using it for ML. Last but not least, bring your human insight to the problem. Recall and re verbalize a reason to all of our responses for what makes it a good feature or not. You need to have a subject matter expertise and a curious mind to think of all the ways that you could construe a data field as a feature. Remember that feature engineering is not done in a vacuum, after you train your first model, you can always come back and add or remove features for Model Number 2.

# Exploring and Preprocessing Data
As a reminder, we're still in the create the dataset phase of our ML model journey. In parallel with your feature engineering process, we want to explore and process our data before we feed it into our model. Now let's examine some of the ways that we can do that. Recall from the first course in the specialization where you're introduced with a few ways to explore your datasets. Now what I do want to mention are two quick points number one, the dataset creation process itself is iterative. The ML model building process does not mean that we explore clean and transform data table into big query and then we're done with all of our model inputs. You'll often find yourself iterating back to your dataset to create and clean new features filled throughout bad values and retry your model to see if your performance improves. Number two, this list is not exhaustive. If you're looking to become a data scientist in the future, you'll find other ways to explore distributions in common data values likely using IPython notebooks with pandas data frames or using something from R in an R library. So all I recommend you start with an approach that works for you and always remain skeptical and curious about your data so

# Other Tools for Creating Data Pipelines
Now before we close off the topic of pre-processing, I just want to mention a few other pre-processing and pipeline tools that you should at least be aware of. We've worked a lot with Cloud Dataprep on batch data in our labs, and recall it creates a Cloud Dataflow job behind the scenes that you can monitor. If you wanted more control and flexibility of your pipelines, especially if you're working a lot with streaming data, writing the actual Java and Python code for your Cloud Dataflow job is always an option. If you're eager to learn how to do that, check out our data engineering on Google Cloud Platform specialization. Lastly, there's a new tool on the Google Cloud Platform called Cloud Composer that'll help you orchestrate your workflows and pipelines, so you don't have to use things like cron jobs or App Engine to schedule jobs. It's essentially a manage to open source Apache Airflow Instance, we can code up your own workflow. Now provide more resources on that if you want to check it out.

# Knowing the Unknowable
Here's one of the most important lessons about creating your dataset for machine learning and it's a little bit philosophical. Let's recap on what we're trying to do here. The whole point of building a model is for it to make predictions and inferences about data that it hasn't seen yet. Like forecasting future sales or classifying new customers. And apart from inventing a time machine, how can we know if our model performs well in data from the future, if we don't have any data from the future. Before we claim all hope is lost. Let's remember what we do have. Thanks to our exploration and pre processing phase, we've gotta clean dataset. You've probably got a label which has the correct answer of ultimately what we're trying to predict for that future data. So some of you might be asking, what's the hold up. We have an entire clean dataset available, feed it to the model, let it learn from it already. Well, no, we can't do that yet. Feeding all that data to the model now gives the model all of our records and the correct answer of the label for each record. While the model might be super happy to have all this data to train on, what happens when it's done training and it's seen all of your data? Can we then just feed in the entire same dataset again but remove the label so it doesn't have the answer? No, it's too late. It's already seen everything. And model the behavior based on all of your available data. Okay, so that leaves us with a big philosophical question that needs answering. How can we one, train the model where you can see the correct answer or the label, because that's to figure out ultimately the recipe to go from your feature columns to that label or model it. And two, also provide the new dataset where it can validate whether or not it did a good job modeling, that relationship that had just learned.
Play video starting at :1:39 and follow transcript1:39
Now, if you remember nothing else from this lesson, here's your huge takeaway. Split your data. If you hold out or don't show all the data to the model at once by splitting it into say 80% training data, 20% validation data, then you can train on 80% of your data and validate with the other 20%.
Play video starting at :2: and follow transcript2:00
Why is validation important. Well, validation will essentially tell your model when to stop training. During training, your model is trying to measure their model, the relationship between your feature columns and you're correct answer column, our label. And sometimes it can do too good of a job and learns relationships that are only found in your training dataset. For example, after too much training, your model could learn that if a customer visits your website on a Friday and they exactly have three page views while having two products in their car, then they'll always come back on a Monday and purchase one of those products in the car and abandoned the other. Well, this seems like an amazing, impressive insight. It's hardly what we call generalizable or applicable to the entire population that we're interested in. Now some of you might be rightfully wondering, how does retraining the model work if it's seen the data already in test and validation? Well, you can performance tune your models knobs which are called hyper parameters and retrain it all against your validation dataset. But at some point, we need to make a go or no go decision to productionalize the model after it sees some real production and future data. You might say, hey, we got ourselves in the same predicament after the model is seen and validated against our validation dataset. How do we get another unbiased opinion about whether or not the final performance tuned model is ready or not for the real world. Well, what do we do before? What are the three key words? Split your data or in this case split your data again. We have the option to split our dataset again into a third split that's called test. Running your performance to an invalidated model against your testing dataset is what I like to say it's the point of no return. You split off your testing data that's never ever seen in training or validation and here's the critical point. You can only use it once as your final go or no go decision. If your model performs well here, if it performs well against this test dataset, it's good to productionalize. If not, then you need to go back to the drawing board and start over completely and potentially collect more data that the model hasn't seen, because again at this point it's seen training, validation and testing data.
Play video starting at :4:12 and follow transcript4:12
Now, after all this we still got two unanswered questions. Number one, how do I actually split my dataset, with what tools? How do I do that? And number two, how do I know whether or not my model is performing well? What metrics can I compare? Well let's tackle each in turn. We'll take the first question of splitting your data set in a repeatable fashion in your very next lesson.

# Creating Repeatable Dataset Splits
Now, in this lesson we'll focus on the practical and repeatable way to sample, and split your dataset using BigQuery. If you've been itching to get back into BigQuery, this lessons for you. Before we discuss splitting our dataset, we first got to find out to split. Now for this example in this walkthrough, we'll use the airline on time performance data from the US Bureau of Transportation Statistics. Google has made this public data available to all users in BigQuery, and we'll call it the airline on time data flights dataset. The dataset has tracked the arrival and departure delays, for over 70 million US flights. So let's demo, how we can effectively sample training validation and testing dataset from this dataset, in a uniform and people way to get our splits. First, you might be asking can't we just use where clause, I know sequel and pull 80% of the rows out. Could we use a random sample each time? Well not quite, a simple random function like the one that you see here on the flights data. We'll just grab a new set of five randomly selected rows, each time you run the query. This makes it very hard to identify, and split the remaining 20% of data for validation and into testing. In addition the data that might be sorted already, which can present some additional problems. Now, for machine learning you want to a repeatable sampling of the data that you have available in BigQuery. And one way to achieve this, is to use the last few digits of the hash function on the field that you want to split your data on. One such publicly available hash function in BigQuery is called farm fingerprint. It just returns the same value anytime it's a vote on a data field. This is just a preview of the critical topic of datasets sampling for Ml, which is explored in much greater depth than our other data engineering. And Ml and GCP online courses, which I encourage you to take. Now, what if someone like you or your colleagues wanted to train your model or retrain it? What would you have to do, with the same training data? Could you still use it? If you retrain your model and got better performance, was because you did a better job tuning the model that I was optimizing it. Or because, the underlying dataset changed completely because it was split randomly. That's the bad news, you can't really know for sure. So, the key takeaway is this splitting your data set must be repeatable.

# Introducing BigQuery Machine Learning

Now it's time for the exciting topic. How do you do machine learning and BigQuery? Before we go into the syntax of model building with SQL. Let's discuss very quickly how BigQuery ML came about. As you saw from the earlier ML timeline doing machine learning has been around for a while, but the typical barriers have been number one, doing ML and small datasets in Excel or Sheets and iterating back and forth between new BigQuery exports. Or if you're fortunate to have a data science team in your organization, number two is building time-intensive tensor flow or psychic learn models using experts time and still you're just using a sample of data that the data scientists can train and evaluate the model locally on the machine if they're not using the cloud. Google saw these two critical barriers getting data scientists and moving data in and out of BigQuery as an opportunity to bring machine learning right into the hands of analysts like you who are already really familiar with manipulating and preprocessing data. You soon will be by the end of this specialization. Okay, so here we go. Let's talk about how you can now do machine learning inside a BigQuery using just SQL. With BigQuery ML you can use SQL for machine learning. And as to repeat that point SQL, No java or python code needed just basic SQL to invoke powerful ML models, right where data already lives inside of BigQuery. Lastly, the team has hidden a lot of the model knobs like hyperparameter tuning or common ML practitioner tasks like manual one, hot encoding of categorical features from you. Now those options are there if you want to look under the hood but for simplicity, the models will run just fine with minimal SQL code.
Play video starting at :1:25 and follow transcript1:25
Here's an example that you'll become very familiar with in your next lab. You notice anything strange about the number of GCB products used to do ML here, you got it. It's all done right within BigQuery, data ingestion, preprocessing with SQL, model training, model evaluation, the predictions from your model, and the output into reporting tables for visualization. As we mentioned before BigQuery ML was designed with simplicity in mind but if you already know a bit about ML you can tune in and adjust your models hyperparameters like regularization, the dataset splitting method, and even the learning rates for the models options. We'll take a look at how to do that in just a minute.
Play video starting at :2:1 and follow transcript2:01
So what do you get out of the box? First BigQuery ML runs on standard SQL and you can use normal sequence and tax like UDFs, subqueries and joints to create your training data sets. And for model types, currently you can choose either from a linear regression model for forecasting or binary logistic regression for classification. As part of your model evaluation, you get access to fields like the ROC curve as well as accuracy precision and recall. They can simply select from after a SQL model was trained and if you'd like you can actually inspect the weights of the model and performed feature distribution analysis. Much like normal village visualizations using BigQuery tables and views, you also can connect your favorite BI platform like data studio or looker and visualize your model's performance and its predictions.
Play video starting at :2:45 and follow transcript2:45
Now the entire process is going to look like this. First and foremost we need to bring our data into BigQuery if it isn't there already, that's the ETL. And here again, you can enrich your existing data where else with other data sources that you ingest and join together using simple SQL joints.
Play video starting at :3:1 and follow transcript3:01
Next is the feature selection and preprocess except, which is very similar to what you've been exploring so far as part of the specialization. And here's where you get to put all of your good SQL skills to the test and creating a great training dataset for your model to learn from. After that, here it is, this is the actual SQL syntax for creating a model inside a BigQuery. It's short enough that I could fit it all within this one box of code. You simply say create model, give it a name, specify mandatory options for the model like the model type passing your SQL query with the training dataset and hit run query, and watch your model run. After your models trained, you'll see that as a new dataset object that will be there inside of your BigQuery dataset and it'll look kind of like a table but it'll perform a little bit differently. because you can do cool things like executed ml.evaluate query and that reserves into actual allow you to evaluate the performance of your trained model against your evaluation dataset because remember you want to train on the different data set that you want to evaluate on. And here you can analyze lost metrics that will be given to you like the root mean squared error for forecasting models and the area under the curve accuracy precision and recall for classification models like the one that you see here. And once you're happy with your model's performance again, you can kind of interest and train multiple models and see which one performs the best, you can then predict with it with this even shorter query that you see here. Just invoke ml.predict and that command on your newly trained model will give you back predictions as well as the models confidence in those predictions. Super useful for classification. You'll notice a new field and the results when you run this query, where you'll see your label field with the word predicted added to the field name, which is simply just your model's prediction for that label. It's that easy. But before we dive into your first lab now that you've seen with just these lines of code that you see here, how easy it is to create a model that doesn't mean that it's going to create a great model. A model is only as good as the data that you feed into it for you to learn the relationship between your features and the label. That's why you're going to spend most of your time exploring, selecting, and engineering good features, so that we can give our model the best possible data set for it to work and learn from.

# Phases of Model Building
Now that we've got are cleaned and split dataset, its time to build the model. Let's walk through exactly what that involves. Here are the five high level steps that we're going to walk through. First, we need to review our goal or objective. What are we trying to do? What are we trying to predict? Then, before we create the model, you want to establish a benchmark for success. Third, you'll select that model type that we want to use. Fourth, as your model gets trained, it'll start outputting performance metrics called lost metrics, which will then use to know if our model is as accurate as it can be. Lastly, we'll tune the model and re-train it, until we're happy with its performance.
Play video starting at ::38 and follow transcript0:38
So let's take a specific ecommerce example and establish an objective. Let's say we want to forecast the amount of monthly site visits to our ecommerce website, so that we can better plan when to run certain online promotions. Next, even before you work with any models, let's establish that benchmark for success. Here, we'll specify a certain amount of tolerance for error in our model's prediction, say + or -100 or 1000 visits per month. We then use this benchmark to assess whether or not the model is performing, as well as, we would like it to. Can we use the model or not? Then, we're finally ready to choose a model type, because we're predicting a numeric field, number of site visits. What type of model should we try out first?
Play video starting at :1:20 and follow transcript1:20
If you're thinking linear regression, that's an absolutely great starting point for forecasting. After the model is trained and evaluated, we'll then the interview those lost metrics from the model to see if we're comfortable with the model accuracy against our performance benchmark that we set. MSE, mean squared error or RMSE, root mean squared error, will be our lost metrics for linear regression. Recall that taxicab example that we demoed earlier, we got a plus or minus $12 on cab fare discrepancy first off, and we hinted that maybe more feature engineering may be necessary. Lastly, after your model is trained and evaluated, you can adjust those model knobs called hyperparameters in additional pre-processing, like add new features or filter out bad data, before retraining your model again. Honestly, you'll find that MLS is as much as an art form here as it is in a science, to approve your performance.

# AutoML Tables
Hi, I'm Michael Abel a big data machine learning technical trainer here at Google, in this video we will see a demo of using AutoML Tables to create a custom neural network regression model without any code. So what we'll do is do some sample pre-processing on the data via writing a query, and then point AutoML to where the pre-processed data lives. AutoML will take over from there. Training our model using neural architecture search or NAS and finding the best model for our data, after our model was trained, it's very simple to evaluate your model's performance and to serve predictions to clients. So let's go through that step by step. So to start off to get our data, let's go to big query and in big query we're going to run a query, which I'll open up here and this is a query on the New York City taxi and limousine commission public dataset for yellow cab rights. So our goal is to build a machine learning model to predict the fair amount which a customer would pay. When we say fair amount here, what we mean is the sum of the base fare and any tolls that a customer would have to pay. So in short, we want this amount to represent how much a customer is obliged to pay that will not include tips. Next, what features do we want to use? I live in the New York City area and as you would most likely guess the time of day and the day of week really affect how much traffic there is. So let's use both day of week and hour day as features, and we can see that here in these two lines. So here we have extract day of week from pickup date time and extract our from pickup date time. Also, the location where you're being picked up and being dropped off will matter. So we'll take the longitude and latitude for both locations to pick up and drop off, and we can see that in these four lines here, once again, we're taking this from a public dataset. The New York City taxi and limousine commission yellow cab dataset. And finally, we're doing some filtering here to be sure that our data is prepared for machine learning. First, New York City yellow cabs have their fare start at 250 the moment that you sit down. So we want to filter out any rides that have a fare amount less than 250. Likewise, we want to be sure that the ride actually occurred, that the person was taken somewhere. So let's also filter out rides which have a trip distance of zero or less, in case that somehow got reported. Finally, we want to be sure that the rides actually occurred in the New York City area. So if we scroll down here, we can see we're more or less putting a rectangle around New York City by specifying pickup longitude, drop off longitude pickup latitude and drop off latitude within certain ranges. Finally, we will be sure that there was a passenger in the car during the ride. So we'll set the passenger count to be greater than zero. So the last thing that we should mention with this query is that we want to take a sample of the data set. The entire dataset consists of over a billion rows of data. So for that reason we're going to take a small sample, roughly, 15000 of the data using this where this part of the where clause here. So finally, let's run this query and save the results.
Play video starting at :3:37 and follow transcript3:37
So what we will do here is we will save the results. So let me click on save query here just so we have it for later and then finally we will save the results as a view. So I'll give a name to this view that will remember, I will call it taxi underscore pre-processed, will reference this in just a moment.
Play video starting at :4:5 and follow transcript4:05
So now we have our data prepared for AutoML tables. Let's jump over to AutoML tables and start the training process.
Play video starting at :4:15 and follow transcript4:15
So once we're at the AutoML table screen and as a reminder, if you need to get here, what you can do is go to the navigation menu, scroll down to artificial intelligence and click on tables.
Play video starting at :4:34 and follow transcript4:34
So let's add our new data set. So maybe I'll call this New York City taxi fare.
Play video starting at :4:54 and follow transcript4:54
So now we have three options for importing our data. We can import the data directly from big query which is what we will do. You can upload a CSP file from Google cloud storage. Also, you can upload files directly from your own computer. In this case, we have our data saved in a big query view. So all we need to do is simply put in the.
Play video starting at :5:20 and follow transcript5:20
Project name.
Play video starting at :5:23 and follow transcript5:23
The dataset ID which my dataset was called demos and the table or view ID. So remember, we called our view taxi pre-processed and hit important. Unfortunately, it can take a little bit of time to import the dataset. It can take as it says here up to an hour. So fortunately I have pre-prepared here another data set where the data has been loaded. So let's go to that.
Play video starting at :5:54 and follow transcript5:54
That way we don't have to wait. So once we've loaded our data into auto ml tables, we see that it automatically detected the data type for each field if no values appeared. And we have a few other things that we need to select before we're ready for machine learning training, such as what are we trying to predict? What is our target or label? The first thing I want to do is it will actually make more sense to treat hour of day as a categorical variable rather than a numeric variable. And the reason for this is that for our of day counting from zero to 23. Well, after 23 come zero not 24, the magnitude of the hour doesn't necessarily make sense. So let's treat it as a categorical variable instead. Let's select our target column which for us will be the fare amount and then for the additional parameters we won't be taking advantage of this here, but I do want to point these out. So by default, your data will be randomly split where 80% of your data roads are for training, 10% for validation and 10% for testing. You can also do a manual split where you add in a column to your data saying is this in training, validation, or testing. However, we'll use the automatic split here. We can also choose a weight call. So that is, if you want certain examples that have a higher weight when training your model than others, this is where you could do that. We don't have a weight column here, so we will leave this as blank. Likewise, you can also include a time column if your data rows are ordered by timestamp. Once again, since we don't have that, we'll leave it as it is, then we can hit continue. Once we hit continue, AutoML tables will analyze our data. So let's give it a moment, and then we'll take a look and see what statistics it generates. We can see statistics here for every column, such as number of distinct values, correlation with the target and other statistics such as the mean and standard deviation for any numeric features. So we can see that here, for example, for a fare amount, we can see the average fare amount and the standard deviation.
Play video starting at :8:25 and follow transcript8:25
So after we looked through all of this and be sure that everything makes sense, we're ready to train, so we can click on the train button to get to the training menu. And we can enter a number between one and 72 for the maximum number of note hours we want to use for training if we hover over this little help button here, there are some recommendations that are given. We have a little bit more than 200,000 rows of data in our sample. So the recommendation here is one to six hours. Let's go ahead and set our budget here for six hours and then say, why that's not a bad idea.
Play video starting at :9:7 and follow transcript9:07
So a few other options to consider, we can select which feature columns we definitely want to include and any target, weight or split columns are automatically omitted. In this case I want to use all six features, but if we decided after analysis, well, maybe we want to throw out hour day, we don't want to use that for training. We don't need to upload a new data set. We can simply be selected here. However, once again we do want to use that as a feature. So let's be sure to include it.
Play video starting at :9:39 and follow transcript9:39
Also, if we go down here to advanced options, we can choose the optimization objective. We will stick to root mean squared error or our RMSE, but you see here that both mean absolute error and root-mean-square logarithmic error are available as well. Finally, remember I made a comment that, we can set it to six hours there's no real detriment. Well, why is that? That's because AutoML can use early stopping. The idea of early stopping here is if the trials of AutoML are not improving the model, then it will halt training so that you don't waste your training time and those additional hours that we're not used will be refunded. So now let's hit train model.
Play video starting at :10:28 and follow transcript10:28
And as you may guess, we set this for six hours. It will take around six hours. So let's go to a different dataset where the model has already been trained.
Play video starting at :10:44 and follow transcript10:44
So we go back to the train screen for this model here, I have two different models that have been trained. So let's look at this first model here, and then we'll come back to the second one in a moment. This model was trained for just one note hour, remember before we set the budget to six, for this model, I set it to one. At the top here we see some information about the model. We see a reminder of the target variable fare amount, which feature columns were included. So we can hover over this and see all six included columns and our MSC was our objective to the right of this, we can see our metrics in particular the one that we want to optimize for our MSC was 3.772 for this model. Here we can see other metrics as well such as mean absolute error root, mean square logarithmic error, Pearson's r squared statistic or correlation statistic and mean absolute percentage error. Another neat feature of AutoML tables is we can see feature importance. We won't go into full detail here but roughly speaking feature importance is computed by measuring the impact that each feature has on the prediction. And we do this by perturbing the values of that feature across a wide spectrum of values from the data set. In AutoML tables the feature importance is are normalized to where they sum up to 100% or one. So on the screen here we can see that even though hour of day and day of week made sense as what seemed like very important features. To help measure how much traffic there is trying to catch a rush hour traffic which would increase taxi fare notice that for a model, the feature importance was very small. The drop-off location latitude and longitude and the pickup location longitude and latitude once again were more important.
Play video starting at :12:50 and follow transcript12:50
Once our model is trained, and we have evaluated our model, then we can predict using are now trained model in a few different ways. So let's go over to the prediction tab here. So we can use batch prediction. The idea of backs prediction is that we can upload data directly from big query or CSP files from Google cloud storage. Compute predictions for the rows of data in those tables and then output the results into either another big query table or a Google cloud storage bucket. This is perfect for when you don't need predictions immediately, and you're working with accumulated data. If you wish to use online prediction where you're able to predict using your model on demand, we need to deploy the model to the cloud. So we click here on online prediction, we need to deploy the model. So that's the point of model, we click on deploy model.
Play video starting at :14:1 and follow transcript14:01
And once again this takes a little bit of time, around 20 to 30 minutes at most to deploy your model. So let's go over to our last model that I mentioned before that I trained to models which is already deployed.
Play video starting at :14:16 and follow transcript14:16
Now, for our deployed model let's get a prediction, we can either use the web UI for one of predictions or make a rest API calls here, we will simply use the web UI. So if we scroll down here, we already have our values for our features, put in what we're going to predict here is if we're taking a yellow taxi cab ride from the Google office in the Chelsea neighborhood of Manhattan. So we can see the pickup latitude and longitude for that location. How much would it cost to go to JFK Airport's Terminal five? Once again, the latitude and longitude are already put in here, on a Thursday afternoon at 5 PM. Let's hit predict, and we see our prediction result, it's predicted that it will cost 52 dollars and nine cents before 10. Once again being from New York City, this may be a little bit low, but if you look at the 95% prediction interval, another feature of AutoML predictions, most likely the higher end of this is more correct.