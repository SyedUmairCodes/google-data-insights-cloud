# Introduction to Machine Learning
Machine learning is a discipline inside of artificial intelligence. The term AI was originally coded in 1956, and has since evolved to encompass many fields of study that are commonplace in today's technological discussions. Topics like machine learning, natural language processing or NLP and computer vision all fall under the modern umbrella. That is AI. Now, if you're thinking of AI as killer robots to take over the world, your field of interest is more HLI or human like intelligence. You'll notice that deep learning is yet an additional sub discipline within machine learning. And it's captured a lot of attention as it has started to even rival and surpass the human ability to perform complex tasks like image recognition, speech recognition, language translation and much, much more. Now, that we know when AI ML and deep learning came about how about a one sentence definition of ML? ML And its core labels things for you. Show a model, a bunch of good historical sales trends data for your clothing store and a model can predict next month's sales. Show a model, lots of photos of cars and what the correct make and model is with enough examples. The model will classify new unlabeled car photos for you. Let's dive a bit more with an example. On the weekends you'll likely find me scouting the web for new sci fi movies and tv series to watch. Now, I can tell you what previous sci fi movies I like and I can do a decent job of narrowing down the list of potential new options myself, but I don't have all the time in the world is scanned through and classified good sci fi movies to watch? I do have a general intuition of what I like, which is also reflected in my history of movies watched. So, things are, it has to be set in space in the near future. It's shorter than two hours and there's no crazy aliens or no horror or anything like that. And I can provide you the list of movies that I also liked. We can then train a model to label. In this case, it's classifying whether or not I like a new sci fi movie or a series that comes out which is about space drama. The key difference though is that although I have an intuition of what I like and I'm providing you the model with a list of movies that I liked and didn't like. I'm not providing the model with a hard coded recipe from narrowing down the movies like if it's shorter than two hours, then prioritize the space movie, as long as it's not horror, that sort of thing. The beauty of ML is that it comes up with this recipe by itself based on the correctly labeled examples that has seen so far. Now, imagine if I didn't provide any insight behind my movie selection process other than all the movies and the tv series in the past, would you have any basis to even build those hard coded rules. Not anymore. And what if I asked you to predict across all genres of movies, which could have very different aspects. How could you maintain a rules based on things like if comedy and actor equals John, else if not horror and duration, less than two hours. All that gets unwieldy. Let the machine learning model figure out the recipe that ties your historical labeled data to the predictions on unseen data. Now, let's extrapolate this to a real world application. Let's take Google search for example, say you go to Google and you search for giants, what should we show you as your results page to make the most relevant for you? Well, if you're in California like me, should we show you the results for the san Francisco giants? It's a baseball team, maybe list some local games nearby. What about if you're based in New York, should we tailor the results to show the New York giants football team instead, as one of the rules? Well, up until a few years ago, this is exactly how Google's search worked. There were a ton of rules that were part of the search engine code base to decide which sports team to show and where based on where the user was. If the query is giants and the users in the bay area, show them the results by the SAN Francisco giants? If the users in the New York area, show them the results about the New York giants. And if they're anywhere else, show them the results about tall people or giants. Those of you have worked with sequel before. Just imagine how many case statements this would be and how hard it would be to maintain and that's just for one query. Multiply this by the large variety of queries that people make, where they make them from, what device they're on. You can imagine how complex the whole code base had become. The code base was getting really unwieldy. Hard coded rules are hard to maintain and this is exactly where ML comes into play. It scales much better because it requires no hand coded rules and it's all automated. Our data set in this case is we know that historically in the search engine result pages, which links that people clicked on. Why couldn't we just train an ML model to provide input into the search ranking and that's exactly what Google itself has done internally. And they used a deep learning ML model called ranked brain. After logging out, the quality of search engine results improved dramatically with the signal coming from rank brain becoming one of the top three influencers for how results are ranked. If you're interested, I'll provide a link where you can read more about it. Not to recap, machine learning, want to lead with examples, not with instructions, any business applications where you have these long case statements of if then else and logically hard coded all that stuff together. But, you do have a history of good labeled data. That's a possible application for machine learning. Now, deep learning, remember that's that sub discipline of machine learning is useful for when we as humans can't even map out our own tuition about what makes a prediction correct or not. So, what do you see here? Now, your eyes and your brain have the benefit of many, many, many years of evolution and intuition to allow you to perceive and interpret all those pixels on the screen. How could we teach a machine to understand that this picture here is a cat? If you let you follow yourself back into the rules making bad habits that we want to try to avoid. You might say well look for a cat like eyes and these images. Okay, what about this image? [LAUGH] Your brain still knows it's a cat but the machine now is no basis to go off of it with an old rule of just look at the eyes and determine if it's cat like eyes. Okay, what happens if we added a bunch more hard coded rules like this? Look for the ears, the eyes and the nose.
Play video starting at :5:53 and follow transcript5:53
Alright, is this still a cat? What about this? Again, You get the point. Hard coding rules completely fails us here and that's where deep learning comes into play. When we just have labeled examples and we completely let the model figure out how to build a good recipe to answer the question, what is a cat? And in 2012, that's exactly what the Google Research team with Jeff Dean and Andrew Ng did. What you see here is what the deep learning neural network figured out what a cat is, based on looking at over 10 million images and processing the model over 16,000 computers.
Play video starting at :6:32 and follow transcript6:32
Now, a familiar architecture for Deep learning is the neural network, which is the model inspired by our own human brains, here it takes the input image that you see there and classifies it as a cat or a dog. And again, we're not telling the model to focus on looking for dog collars or cat whiskers. It builds its own recipe for determining the correct label and applies it to the end. As you can see from the image, modern ML models can scale and handle even tricky data points like this dog hiding in the laundry basket.

# Demo: Google Photos
Machine learning, specifically deep-learning, they are the core of many Google products, like Google Photos, which can classify and group photos, like photos of your pets together automatically in an album. Note that this is now multiple machine learning models in one. First needs to identify whether or not the photo you just took is of a dog. Second, it needs to identify whether this dog is your pet based on comparing this photo and the history of photos you have with that specific dog. Now let's take a quick demo of Google Photos. Nothing excites me more than talking about machine learning where you can use a real life application and especially one where I get to show you all these cool photos of my dog Rex. Here I'm going to hop over into Google Photos, which is photos.google.com. It's one of the coolest products I think, just because there's a lot of behind the scenes "magic". But now that you know what machine learning is, it's not really magic, just really highly performance tune models. You can see it. We can do some cool stuff. Here's some photos, just moved the houses recently, you can see I've got some house photos and photos of my dog and videos of my dog here as well. What we're going to try to do is showcase some of the cooler features of things that you can do ML. One of the things that you can do, which I really like since I take a lot of photos, is an automatic classification. I can just search on all of the photos that I have, searching for things like brown dog, even without me manually labeling all of these because that would just take forever, finds photos and even videos. This is new to me, where brown dogs are present somewhere in the video, which is cool. This automatic classification, the photos team has even taken it one step further working, recognize the difference between say, a brown dog and say my own pet, which happens to be Rex. If I just typed in Rex, somewhere along the way it'll process and basically say, "Hey, we noticed you're taking a lot of these photos of the same dog. Do you want to say this is your pet?" It's like, sure, this is your pet. I think it works the same way with your kids as well. Basically it will then learn what your pet looks like, and any new photos that you upload will automatically be added to your pet's album. This is a shameless way for me to show cool pictures of Rex, but it also shows the underlying principles of lots of ML models automatically working as soon as you upload a photo into Google Photos. Give it a try yourself, and then stick around for the rest of the course, we're going to be going into how you can actually invoke some of these image classification APIs to see what's inside of these photos. Then eventually work our way into building some models like classification models ourselves. See you soon.

# What is deep learning?
You'll find deep learning in many of today's applications like self driving cars, where you can imagine the speed and the accuracy of the Ml models prediction is absolutely paramount and real world driving scenarios. Here you can see another example of multiple ML models collaborating together in a single application. The Google translate app, lets you point a phone camera at a street sign, it will translate the sign for you. Now, here, you've got one model to find the sign, another model to read the characters off the sign. A third model to detect the language, a fourth model that translate that sign a fifth model to superimpose that translated text back on the sign and maybe even a six model to select, which fought to use, all happens within seconds.

# ML Applications for Business
Understanding where ML can be applied is the first step towards framing an ML project and eventually building an ML solution. Let's take a look at some common ML use cases you'll find in businesses today. In business operations, you could train and build a model to predict parts failures in cars given historical issues and conditions. Or maybe predict how many customers you can expect to make claims on your warranties based on past issues and trends, which will help estimate sales support staff. For process optimization, consider having IOT devices on the factory floor that you have and it automatically feeds temperature data for the different parts in the assembly line. And allow you to optimize and control that with a model. Google actually did this and apply the ML model to regulate the heating and cooling of its own data center temperatures, which actually reduce cooling costs by 40%. If you work with customers, you can use ML to intelligently flag and route support tickets and calls to the right responders and even draw insights at a natural text like products, sentiment. Using ML models to influence sales is a very common application as you might imagine for e-commerce and retail. You can track and recommend products to users based on what they already have in their shopping carts or past purchases. ML from marketing enables you to automatically track and analyze the sentiment of social media feedback text product reviews and generally have a leading predictive indicator of how your business is perceived and performing. In addition to having e-commerce models effect in store sales, you can predict the lifetime value of a customer based on similar behavior and purchases from other customers. And since you're already familiar with sequel, segmenting customers with aware clause in the field is simple. But add the power of automatically segmenting customers into groups based on historical purchasing behavior and you unlock the power of ML. For finance, a very common application as you might imagine is forecasting demand or forecasting sales for the upcoming quarters. Here you can again imagine that hard coded rules fail. You have something like if the current promotion is live and it's the black Friday sales and it's close to the holiday season and the customer is not purchased before. It's just way too much to maintain. Let the model figure it all out. That leads us into our discussion question. What other business areas or topics could you show a model examples of and have it make inferences or predictions? Now you can either come up with your own or expand on the ones that we've previously mentioned, which I'll show again here.
Play video starting at :2:25 and follow transcript2:25
Now one of my favorites is the marketing ML use case for predicting which customers have the highest return on investment or ROI. And their overall lifetime value to ULTV as a business. Our goal is to better target high value customers throughout our e-commerce product lifecycle. And we want to target those customers that are high value with special promotions and incentives. Natural exploring the data, we can provide a number of useful fields to the model, like the different days that the visitors been to our website. How many lifetime page views they have, total visits, with their average time on the website was in total, the total revenue that they brought in and the count of e commerce transactions on our site like you see here. All that I've shown you here is basic analytics. But you get a sense of the data that you can then feed this historical data this label data into your model and use that to power and predict your ML model. Which will then output which customers are high value and that will help you focus and target your promotions and incentives for that group. But before we get too deep building our models into big query which we will do, we first need to define our data terms in the language that data scientists and other ML professionals use. So let's do that now.

# ML Terms: Instances, Features, Labels
Taking the e commerce example we had in the previous lesson, a record or a row is called an instance or an observation. The image you see here we have eight instances. A label is the correct answer. And it'll be what you're looking to train the model on with your existing data and predict with your model for future data. Here the label is lifetime revenue which is a number will be trying to predict. Labels could also be things like binary values like whether it's a high value customer or not. As you see here, knowing what you're trying to predict a class, a number etcetera. Will greatly influence the type of model that you're going to use later. So we've got the label field. What do we call all these other data columns in our data table? Well, those columns are called features. We have a whole module dedicated to creating ML datasets and big query which touches on the critical topic of feature engineering. Which is essentially exploring cleaning and pre-processing your data before you input it and give it to your ML model. This is often the hardest part of any Ml project and that's why it's great that you already enjoy working with data as an analyst. Now say you have some new data that comes to you that you don't have a label for. We now have a dataset of labeled examples and a data set of unknowns. Well, this is a really fun part. We can draw inferences and predict those values with a model. Again, an ml model will build a recipe for determining those output values. In this case, classifying whether or not that customer is a high value customer or not based on your labeled training data that it already knows the answer to which is in the blue box. Finally, once are labeled training data is in place. We can select which ml model we're going to use to help us with our goal. Here are two lightweight models for us to start with. If you're trying to predict for a label, that's a number like how much revenue do I expect to earn this year? Try using linear regression first. Now on the other hand, if you're trying to classify a particular customer, say high value or not. Considered using a classification model like logistic regression. Now there are many other model types like decision trees, neural networks, random forests and probably more since we created this lesson, the ultimate goal is the same. If we're given labeled training data, predict the label for unseen or unknown data. Now, no topic on Ml models would be complete without first discussing the difference between supervised and unsupervised learning. Supervised learning is everything that we've covered so far. It's where you already know the correct answer as a label in your data set of your training data set. And spoiler alert, these problems are typically easier for prediction. Unsupervised learning, is where you have the data but you don't have any labels. Consider the example of clustering customers based on behavior. Or to generally explore for insights on how the model groups them together, but you're not making predictions.

# The Spectrum of ML Tools
And by the time you watch this lesson there's probably already a new MA model available for training out there in the world. Machine learning and AI are continuously evolving fields of study and practice and a data analyst will likely experience and use ML tools in a different way than a data engineer would or a data scientist would. So let's take a look at some specific ML tools that will use on google cloud platform and who uses which tools.
Play video starting at ::23 and follow transcript0:23
So here is the spectrum of ml tools that you as a data analyst should be familiar with even just in name only. On the left hand side are very low level tools that require programming expertise like python and these are like building a custom TensorFlow model. Now, all the way on the right are tools that require very little programming background or even ML background operate. And from left to right we've got the TensorFlow library ML models within big query, which is the huge focus for this course. Pretrained MLAPI's, which will practice invoking very shortly in a new product called auto ml, which aims to democracy size ML for everybody. So let's take a tour of each of these tools. TensorFlow is an open source numeric processing library that was the result of many years of custom model development and expertise here internally at google. Now made available for the public, open source, it's currently one of the most popular machine learning libraries on Git hub. So in TensorFlow you write python code, invoke ML optimized pre processing code and a variety of pre canned functions like the estimator API for your models and it's platform independent. TensorFlow is beyond the scope of this data analyst course. What's directly in scope of this course and extremely exciting for us. Big Query users is the new ability to train and run ML models directly within big query with the latest feature release. At this time of writing Big Query ML supports linear regression and logistic regression models which conveniently are the two that we're going to cover later during our forecasting and classification topics. And if you don't have a rich labeled dataset but still want to take advantage of machine learning for common applications like image classification, speech recognition or language translation. We'll try out the new cloud Machine learning API's. We'll cover these in depth in the next lesson and you'll experiment with them in your next lab. Last on the spectrum is auto ml, which is where you can use a UI to build an ML model in minutes at the time of this recording. Auto ML vision for image classification is the closest being released to the public. The product even provides a direct integration with Google's human labeling program where if you have images with no labels. Google can provide an in house team of labelers that you can provide instructions to and they'll classify your images for you. And these are the same labelers that google uses for its own products. Now here are just a few of the customer examples of companies who have used ML tools on google cloud platform for their data sets. The left to right ordering here again is reflective of the low level programming all the way on the left to the more abstraction or UI based all the way on the right. Let's take a meeting turn a kitchenette built their own custom model to classify images of car parts and estimate their price. Ocado used parsed results from the NLP or natural Language API to router reroute customer emails to the correct responders and prioritize them. GIPHY uses the out of the box vision API to find the text and memes using optical character recognition or OCR. It can then reject inappropriate uploads based on sentiment or keywords. Unique loan designed a shopping chatbots using a dialogue flow UI. Dialogue flow is a Google owned company which specializes in building ML based interfaces like intelligent chatbots. I'll provide a link to where you can watch more customer stories as well.

# Pre-Trained ML Models
So now that I've got you excited about all the possibilities of using machine learning, let's actually get into the specifics. We're going to start by looking at the world of ML model types, and why some out of the box models have been perfect starting points. Now there are two broad categories of ML models. You have models that are already built, we call them pre-trained and then those custom models that you build yourself using code like TensorFlow. For pre-trained models, Google has already figured out a lot of these hard problems. Things like the vision API that you saw before on images, the speech API which is trained on something like YouTube captions and the machine translation API, everything's like language translation. Now recall that how well your model is trained depends largely on how much data you have access to. As you might expect Google has a lot of images and text and ml researchers to train its pre built models, so you can use those instead of reinventing the wheel.
Play video starting at ::54 and follow transcript0:54
As an example if you're looking to have video captions included in a recent webinar that your company hosted considered using the translator speech APIs. Instead of trying to hand code and build a language recognition ml model for yourself.
Play video starting at :1:7 and follow transcript1:07
Another example, if you have text documents like expense receipts that need to be classified by expense type, consider using something like the cloud vision API for OCR, so you can then mine the text from the receipts and then drop that data into something like big query for analysis. Now all that said, if you do want to go to the custom model route and write some of your own TensorFlow code which is taught in a lot of other future data engineering courses beyond this one. Note that cloud machine learning engineers, you see all the way there on the right, will fully manage and host and run your custom built models. So even if you wanted to write your own code, the cloud can still be a place where you can start and run it. Here's the key point, the ML APIs, we'll be covering throughout this course like cloud vision. They abstract away a lot of the underlying model code and leave you with a simple service that you can provide in that input and receive labeled output. And the bottom line here is that even when we eventually build our own models, interacting with them for things like predictions should be simple scalable and seamless. You ultimately want to provide the end user or whoever is going to be invoking your service or your models are like an A PI. A simple interaction interface like give me input to define the expected parameters and then generate output that meets our our performance qualifications

# Demo: Cloud Translate and Vision APIs
Here's the google translation API. See it expects a simple text input, a few parameters around language and then it will handle the rest for you and then output the result. Now here's a super quick demo of what that actually looks like on the web through column API. If you get a cloud dot google dot com slash translate, you'll be able to see the API that actually translates the languages. And here you see the source input text which is in Chinese, translate the target language into English and if you actually open up translate text, you can see the actual response that's returned. So you might be wondering or asking why do I need an ML model for language translation. Couldn't I just do a dictionary look up and hard code, some rules between the languages, right? Well, as our giants example earlier that you saw in the class, we want to get away from trying to provide these heuristic or hard coded decision trees or rules. And instead rely on the machines themselves to tease out these patterns that we may not have been realized that are there. And to build on this in the last few years, Google's language translation service through the addition of things like the deep neural networks we talked briefly about. Actually made a huge leap over prior versions of a lot of their other language translation systems. It made the translations themselves sound much more like the way people actually speak the language. And what makes the difference is that the system doesn't translate each part of the sentence, piece by piece through a hard coded rule. But it actually looks at the entire sentence as a whole. And this helps the system figure out the broader context in the most relevant translation. And then rearranges and adjust the sentence using things like proper grammar. So it's looking holistically at the entire sentence, right? Trying to build and hard coded rules for every single sentence that's in existence is an impossible task for a human to do. But it's something that comes naturally for machine learning. Okay for a bit of fun understanding let's unpack one of the pre built an LA APIs. Let's take the Cloud Vision one to start. Now there are three major components that all roll up into the cloud Ml vision API. Behind the scenes, each of these is powered by many ML models and years of research. The first is detecting what the image is, and classifying it. For example, if you show them on like cat, it will correctly classify it as a cat. Next what about images of a text on the inside? Like scanned documents. The API will extract the text into a searchable and select able format. Lastly you can gain a little bit of intuition from the web. Does the image contain known entities that we already know of like the Eiffel Tower or famous person? Have it identify those for us. So let's test the limits of each of these with a pretty fun web demo. And then in the lab we'll discuss how to access them programmatically through python. Alright, so first up is this picture of an owl and I don't want to start with the easy image. So I picked one that was a little bit camouflaged inside of that tree there. I don't see why the computer does the recognizing that. So cloud dot google dot com slash vision. Going to find our image. Here's our own dot JPG. I'm going to try this yourself in just a few moments. Let's see what it returns. Alright well there's our image and it did correctly label it as an hour and also picked up a tree. It's also considered a bird of prey, great. So I was also potentially information from the web, excellent. Alright, cool. Let's move on. Let's see if we can't stop it a little bit. So there's the results there. Now what about some text that's embedded but a little bit hard to see? Let's jump back in. Let's get rid of our owl. Let's upload some clipboards.
Play video starting at :3:35 and follow transcript3:35
Let's see. So we got texts In two different places. It labels is as font calligraphy and it's got some wood in there. And let's see if you can pick up some texts. Inside the document you see the bounding boxes there, and recognize that there is some texts and it did pick them up great. So here's the text for block 1, and then block 2 as well. And as the author, great. So as part of this, there's got to be a optical character recognition OCR model that's built into even just recognize that there is text that's pulled onto there as well. Now what about if we know something already about the image like here's Coit Tower inside of San Francisco? Let's see if we can't get it and recognize that. So let's find the image, upload it, see what it says. Off the labels we have things that are generic like this is a city, this is the sky. It's also an urban area. There's a skyline of a city. There's no selectable text that I've seen. Let's see on the web if it recognizes it. Yeah, so based on what it knows and it's seen of other similar images and access to the internet. It says, well, I'm pretty darn sure that this is Coit Tower which it is. As you see those results there. So a lot of different features kind of packed into this one API. That will access through the lab. But before the lab would I encourage you to do go to clown dot google dot com slash vision. Find an image of your choice and try to test the limits of it. Find something that's not easily recognizable maybe. And then upload that there and then see what text is pulled out and see what labels it finds and see what information from the web is there. So give that a try.

# Demo: Natural Language Processing API
Now, let's explore the components of the NLP or Natural Language Processing API, which is yet another machinery model, to see what it expects and then what outputs we can get from it. Now there are a few components that all roll up into the ML language APIs that are available. The first is recognizing the spoken word, which is actually the Cloud Speech API. And after the words are fully understood, they can be translated through full sentences if you remember before, into other languages. Lastly, we parsed the content itself for things like sentiment, parts of speech and other known entities.
Play video starting at ::35 and follow transcript0:35
So let's do a quick demo and see how well the MLAPI actually understands us.
Play video starting at ::39 and follow transcript0:39
So cloud.google.com/speech and you get this box. So here we find ourselves at cloud.google.com/speech and this is the web UI that's going to invoke the Cloud Speech API underneath the hood. But let's see how well we can get this thing to work.
Play video starting at ::57 and follow transcript0:57
Google, headquarters in Mountain View on vaile the new Android phone at the customer Electronics Show. Sundar pichai said in his keynote that users love the new Android phones.
Play video starting at :1:13 and follow transcript1:13
So again, you see this in a lot of different Google products. This is like voice to text, if you're trying to send out a text message or Google hangouts message. But you can think of, if you had this for a webinar or any other spoken text where you just want to capture the user's input. Think of what we can do now that we actually have this data point. So just copy that text, we're going to be using that later on.
Play video starting at :1:36 and follow transcript1:36
Quickly, that's the Cloud Speech API, let's see what else we can do. So now it's time to translate. Here we are in the cloud.com to translate. We're back here again. Let's switch our language. We're going to start in English and we're going to end up in, let's say French. And from my clipboard before, we just have the text and immediately again you see the translation there. Next up, let's take a look at another ML language API. This is the NLP or Natural Language Processing API. And let's see if we can't recognize some entities inside of our text. Here's the API, [LAUGH]. Already guessed this is actually where I stole the text from for the rest of the different demo. So I'm just going to go ahead and click analyze. And here you provide input text and you can see just a variety of things that you can parse out from it. So this is the different entities that it recognizes. It says that Sundar Pichai is a person who goes on Wikipedia article here. Or on Google is an organization, users or people, Android as a consumer good. The location is located in Mountain View. Again, you're pulling out a bunch of different known entities. And here's the text right here, you can see it annotated. This is a sentiment associated with each, this is an announcement. So you hopefully expect a lot of green and positive announcement, where you can see the magnitude, where how positive are those positive words that you're using? So it's a pretty fun or you can kind of compare the score magnitude and we'll play that within our lab as well. If you're an English major or interested in just the parts of speech, you can get those here in the syntax as well. Okay, last up before the lab, it's your turn to play around with the actual Natural Language. And this is where you're going to be using the web interface. And then you can compare the similarities and differences between the using the web interface and then what you're going to do inside your next lab which is actually accessing it through the Python API. So go ahead and navigate to cloud.google.com/natural-language. Scroll down until you find that page, where you can insert the text to analyze it. Type in a sentence or copy in your favorite quote of your choice, click analyze. And at least take a look for the entities, and then look at the magnitude and the sentiment score that you see under sentiment. And then come back in and tune in for the lab.
